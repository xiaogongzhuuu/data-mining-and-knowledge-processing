import os
import math
import re
import pandas as pd
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from collections import Counter

# ä¸‹è½½åœç”¨è¯èµ„æºï¼ˆå¦‚æœå·²ç»å­˜åœ¨ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰
nltk.download('punkt')
nltk.download('stopwords')

# è‹±æ–‡é€šç”¨åœç”¨è¯ + é¢†åŸŸç›¸å…³æ— ä¿¡æ¯è¯ï¼ˆäº§å“ç±»åˆ«ã€æ•°é‡è¯ç­‰ï¼‰
EN_STOPWORDS = set(stopwords.words('english'))
CUSTOM_STOPWORDS = {
    'book', 'books', 'movie', 'film', 'cd', 'dvd', 'album', 'product', 'item',
    'one', 'time', 'story', 'review', 'people', 'game', 'version', 'copy',
    'series', 'music','money','easy','reviews'
}
ALL_STOPWORDS = EN_STOPWORDS | CUSTOM_STOPWORDS

# -------- 1. æ–‡æœ¬é¢„å¤„ç†ï¼šå’Œæ•°æ®å®éªŒ1.py ä¿æŒé£æ ¼ä¸€è‡´ --------

def preprocess_text(text):
    """æ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼Œå…ˆå…œåº•ä¿è¯æ˜¯å­—ç¬¦ä¸²"""
    if text is None:
        text = ""
    elif isinstance(text, float):
        if math.isnan(text):
            text = ""
        else:
            text = str(text)
    else:
        text = str(text)

    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)

    # è¿›ä¸€æ­¥æ¸…æ´—ï¼šåªä¿ç•™å­—æ¯è¯ï¼Œé•¿åº¦>2ï¼Œä¸”ä¸åœ¨åœç”¨è¯è¡¨ä¸­
    clean_tokens = []
    for tok in tokens:
        if not tok.isalpha():
            continue
        if len(tok) <= 2:
            continue
        if tok in ALL_STOPWORDS:
            continue
        clean_tokens.append(tok)

    return clean_tokens


# -------- 2. è¯»å– CSVï¼Œæ„é€  text åˆ—ï¼Œå¹¶æŒ‰æ ‡ç­¾åˆ’åˆ† --------

def load_data(csv_name="test.csv"):
    base_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(base_dir, csv_name)

    df = pd.read_csv(csv_path)

    # ç¬¬ 0 åˆ—ï¼šæ ‡ç­¾ï¼ˆAmazon polarity: 1=negative, 2=positiveï¼‰
    # ç¬¬ 1 åˆ—ï¼šæ ‡é¢˜ï¼Œç¬¬ 2 åˆ—ï¼šè¯„è®ºæ­£æ–‡
    df["text"] = (
        df.iloc[:, 1].astype(str).fillna("") + " " +
        df.iloc[:, 2].astype(str).fillna("")
    )

    # æŒ‰æ ‡ç­¾æ‹†åˆ†ï¼šä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„ç†è§£æ”¹è¿™ä¸¤è¡Œ
    neg_df = df[df.iloc[:, 0] == 1]
    pos_df = df[df.iloc[:, 0] == 2]

    # ä¸ºäº†ç”»å›¾é€Ÿåº¦ï¼Œå¯ä»¥é‡‡æ ·ä¸€éƒ¨åˆ†
    def sample_df(x, n=5000):
        if len(x) > n:
            return x.sample(n=n, random_state=42)
        return x

    pos_df = sample_df(pos_df)
    neg_df = sample_df(neg_df)

    return pos_df, neg_df


# -------- 3. ç»Ÿè®¡è¯é¢‘å¹¶æ ¹æ®æƒ…æ„ŸåŒºåˆ†åº¦ç­›é€‰è¯æ±‡ --------

def compute_class_counters(pos_df, neg_df):
    """åˆ†åˆ«ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬ä¸­çš„è¯é¢‘"""
    pos_counter = Counter()
    neg_counter = Counter()

    for text in pos_df["text"]:
        tokens = preprocess_text(text)
        pos_counter.update(tokens)

    for text in neg_df["text"]:
        tokens = preprocess_text(text)
        neg_counter.update(tokens)

    return pos_counter, neg_counter


def build_sentiment_freqs(pos_counter, neg_counter, sentiment="positive",
                          min_total=30, min_ratio=0.7, min_class_count=15):
    """
    æ ¹æ®â€œåœ¨æŸä¸€ç±»ä¸­æ›´åå‘å‡ºç°â€çš„åŸåˆ™ï¼ŒæŒ‘å‡ºçœŸæ­£å…·æœ‰æƒ…æ„ŸåŒºåˆ†åº¦çš„è¯ã€‚

    - sentiment: "positive" æˆ– "negative"
    - min_total: è¯¥è¯åœ¨æ­£è´Ÿæ ·æœ¬ä¸­æ€»å‡ºç°æ¬¡æ•°è‡³å°‘ä¸º min_total
    - min_ratio: è¯¥è¯åœ¨ç›®æ ‡æƒ…æ„Ÿç±»åˆ«ä¸­çš„å æ¯”è‡³å°‘ä¸º min_ratio
    - min_class_count: è¯¥è¯åœ¨è¯¥ç±»åˆ«ä¸­çš„å‡ºç°æ¬¡æ•°è‡³å°‘ä¸º min_class_count
    """
    freqs = {}

    all_words = set(pos_counter.keys()) | set(neg_counter.keys())
    for w in all_words:
        pos_c = pos_counter.get(w, 0)
        neg_c = neg_counter.get(w, 0)
        total = pos_c + neg_c
        if total < min_total:
            continue  # å¤ªç¨€æœ‰çš„è¯è·³è¿‡

        if sentiment == "positive":
            if pos_c < min_class_count:
                continue
            ratio = pos_c / (total + 1e-9)
            if ratio < min_ratio:
                continue
            freqs[w] = pos_c
        else:
            if neg_c < min_class_count:
                continue
            ratio = neg_c / (total + 1e-9)
            if ratio < min_ratio:
                continue
            freqs[w] = neg_c

    return freqs


def plot_wordcloud(freq_dict, title, out_file=None):
    """åŸºäºé¢‘ç‡å­—å…¸ç»˜åˆ¶è¯äº‘ï¼Œåªå±•ç¤ºç­›é€‰åçš„é«˜åŒºåˆ†åº¦æƒ…æ„Ÿè¯"""
    if not freq_dict:
        print(f"âš ï¸ è¯é¢‘å­—å…¸ä¸ºç©ºï¼Œ'{title}' æ— æ³•ç”Ÿæˆè¯äº‘ï¼ˆç­›é€‰æ¡ä»¶å¯èƒ½è¿‡ä¸¥ï¼‰ã€‚")
        return

    wc = WordCloud(
        width=1000,
        height=500,
        background_color="white",
        max_words=200
    ).generate_from_frequencies(freq_dict)

    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(title)
    plt.tight_layout()

    if out_file:
        plt.savefig(out_file, dpi=300)
        print(f"âœ… å·²ä¿å­˜è¯äº‘åˆ°: {out_file}")

    plt.show()


def main():
    print("ğŸ‘‰ æ­£åœ¨åŠ è½½å¹¶æ‹†åˆ†æ­£è´Ÿæ ·æœ¬...")
    pos_df, neg_df = load_data("test.csv")
    print(f"  Positive æ ·æœ¬æ•°: {len(pos_df)}")
    print(f"  Negative æ ·æœ¬æ•°: {len(neg_df)}")

    print("ğŸ‘‰ ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬è¯é¢‘...")
    pos_counter, neg_counter = compute_class_counters(pos_df, neg_df)

    print("ğŸ‘‰ æ ¹æ®åŒºåˆ†åº¦ç­›é€‰ Positive æƒ…æ„Ÿè¯...")
    pos_freqs = build_sentiment_freqs(
        pos_counter,
        neg_counter,
        sentiment="positive",
        min_total=30,
        min_ratio=0.7,
        min_class_count=15,
    )

    print("ğŸ‘‰ æ ¹æ®åŒºåˆ†åº¦ç­›é€‰ Negative æƒ…æ„Ÿè¯...")
    neg_freqs = build_sentiment_freqs(
        pos_counter,
        neg_counter,
        sentiment="negative",
        min_total=30,
        min_ratio=0.7,
        min_class_count=15,
    )

    print("ğŸ‘‰ ç”Ÿæˆ Positive è¯äº‘...")
    plot_wordcloud(pos_freqs, title="Positive reviews word cloud (discriminative)", out_file="wordcloud_positive.png")

    print("ğŸ‘‰ ç”Ÿæˆ Negative è¯äº‘...")
    plot_wordcloud(neg_freqs, title="Negative reviews word cloud (discriminative)", out_file="wordcloud_negative.png")


if __name__ == "__main__":
    main()