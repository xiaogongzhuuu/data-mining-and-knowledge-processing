import os
import math
import re
import multiprocessing
import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# ç¡®ä¿ tokenizer å¯ç”¨
nltk.download("punkt")

# å…¨å±€å˜é‡ï¼Œç”¨äºå­è¿›ç¨‹å…±äº«æ¨¡å‹
global_model_wv = None

def init_worker(model_path):
    """å­è¿›ç¨‹åˆå§‹åŒ–ï¼šåŠ è½½æ¨¡å‹"""
    global global_model_wv
    print(f"ğŸ”§ å­è¿›ç¨‹ {os.getpid()} æ­£åœ¨åŠ è½½æ¨¡å‹...")
    # åªåŠ è½½ KeyedVectors ä»¥èŠ‚çœå†…å­˜ï¼ˆå¦‚æœåªéœ€è¦å‘é‡ï¼‰
    # æ³¨æ„ï¼šå¦‚æœ model ä¿å­˜çš„æ˜¯å®Œæ•´ Word2Vec å¯¹è±¡ï¼Œload åå– .wv
    model = Word2Vec.load(model_path)
    global_model_wv = model.wv
    # é”å®šä»¥é˜²æ„å¤–ä¿®æ”¹
    global_model_wv.init_sims(replace=True)

def preprocess_text(text):
    """å’Œä½ è®­ç»ƒ Word2Vec æ—¶åŒæºçš„é¢„å¤„ç†é€»è¾‘ï¼Œä¿è¯ä¸€è‡´"""
    if text is None:
        text = ""
    elif isinstance(text, float):
        if math.isnan(text):
            text = ""
        else:
            text = str(text)
    else:
        text = str(text)

    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    tokens = word_tokenize(text)
    return tokens

def get_document_vector_worker(text):
    """Worker å‡½æ•°ï¼šè®¡ç®—å•ä¸ªæ–‡æœ¬çš„å‘é‡"""
    global global_model_wv
    tokens = preprocess_text(text)
    vecs = []
    for tok in tokens:
        if tok in global_model_wv:
            vecs.append(global_model_wv[tok])
    if vecs:
        return np.mean(vecs, axis=0)
    # æ²¡æœ‰ä»»ä½•è¯åœ¨è¯è¡¨é‡Œï¼Œå°±ç»™ä¸€ä¸ªå…¨é›¶å‘é‡
    return np.zeros(global_model_wv.vector_size, dtype=np.float32)

def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    w2v_path = os.path.join(base_dir, "word2vec_sentiment.model")
    
    # === 1. åŠ è½½è¯„è®ºæ•°æ® ===
    csv_path = os.path.join(base_dir, "test.csv")
    print("ğŸ‘‰ æ­£åœ¨åŠ è½½è¯„è®ºæ•°æ®:", csv_path)
    df = pd.read_csv(csv_path, header=None, names=["label", "title", "text"])
    
    # åˆå¹¶ title + text ä½œä¸ºå®Œæ•´è¯„è®º
    df["full_text"] = df["title"].astype(str).fillna("") + " " + df["text"].astype(str).fillna("")
    print("ğŸ‘‰ è¯„è®ºæ€»æ•°:", len(df))

    # === 2. è®¡ç®—æˆ–åŠ è½½æ–‡æ¡£å‘é‡ (Caching) ===
    cache_X_path = os.path.join(base_dir, "cache_X_all.npy")
    cache_y_path = os.path.join(base_dir, "cache_y_all.npy")

    if os.path.exists(cache_X_path) and os.path.exists(cache_y_path):
        print("âš¡ï¸ å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
        X_all = np.load(cache_X_path)
        y_all = np.load(cache_y_path)
        print("âœ… åŠ è½½å®Œæˆ")
    else:
        print("ğŸš€ æœªå‘ç°ç¼“å­˜ï¼Œå¼€å§‹å¹¶è¡Œè®¡ç®—æ–‡æ¡£å‘é‡...")
        
        # å‡†å¤‡æ•°æ®
        texts = df["full_text"].tolist()
        labels = df["label"].values
        
        # å¹¶è¡Œè®¡ç®—
        # æ ¹æ® CPU æ ¸å¿ƒæ•°å†³å®šè¿›ç¨‹æ•°ï¼Œä¿ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
        num_workers = max(1, multiprocessing.cpu_count() - 1)
        print(f"ğŸ‘‰ å¯åŠ¨ {num_workers} ä¸ªè¿›ç¨‹è¿›è¡Œè®¡ç®—...")
        
        with multiprocessing.Pool(processes=num_workers, initializer=init_worker, initargs=(w2v_path,)) as pool:
            # ä½¿ç”¨ imap ç¨å¾®èŠ‚çœå†…å­˜ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦
            doc_vectors = []
            total = len(texts)
            for i, vec in enumerate(pool.imap(get_document_vector_worker, texts, chunksize=100)):
                doc_vectors.append(vec)
                if (i + 1) % 10000 == 0:
                    print(f"   å·²å¤„ç† {i + 1}/{total} æ¡è¯„è®º...")
        
        X_all = np.vstack(doc_vectors)
        y_all = labels
        
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç¼“å­˜...")
        np.save(cache_X_path, X_all)
        np.save(cache_y_path, y_all)
        print("âœ… ç¼“å­˜å·²ä¿å­˜")

    print("âœ… æ–‡æ¡£å‘é‡çŸ©é˜µå½¢çŠ¶:", X_all.shape)

    # === 3. æ˜¯å¦æŠ½æ · ===
    N_SAMPLES = 200000  # ä½ çš„ç›®æ ‡æ˜¯ 20ä¸‡
    if N_SAMPLES is not None and X_all.shape[0] > N_SAMPLES:
        print(f"ğŸ‘‰ è¯„è®ºå¤ªå¤šï¼Œåªéšæœºé‡‡æ · {N_SAMPLES} æ¡ç”¨äº t-SNE")
        idx = np.random.choice(X_all.shape[0], N_SAMPLES, replace=False)
        X = X_all[idx]
        y = y_all[idx]
    else:
        print("ğŸ‘‰ ä½¿ç”¨å…¨éƒ¨è¯„è®ºåš t-SNE")
        X = X_all
        y = y_all

    print("ğŸ‘‰ å‚ä¸ t-SNE çš„è¯„è®ºæ•°:", X.shape[0])

    # === 4. t-SNE é™ç»´ (ä¼˜åŒ–å‚æ•°) ===
    tsne = TSNE(
        n_components=2,
        perplexity=30,
        init="pca",          # âš¡ï¸ ä¼˜åŒ–ï¼šä½¿ç”¨ PCA åˆå§‹åŒ–ï¼Œé€šå¸¸æ›´å¿«ä¸”æ•ˆæœæ›´å¥½
        learning_rate="auto", # âš¡ï¸ ä¼˜åŒ–ï¼šè‡ªåŠ¨å­¦ä¹ ç‡
        n_jobs=-1,           # âš¡ï¸ ä¼˜åŒ–ï¼šä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒè¿›è¡Œæœ€è¿‘é‚»æœç´¢
        random_state=42,
        verbose=1            # æ˜¾ç¤ºè¿›åº¦
    )
    print("ğŸ‘‰ å¼€å§‹å¯¹è¯„è®ºå‘é‡åš t-SNE é™ç»´ ...")
    X_2d = tsne.fit_transform(X)
    print("âœ… t-SNE å®Œæˆ")

    # === 5. ç”»æ•£ç‚¹å›¾ ===
    plt.figure(figsize=(10, 8)) # ç¨å¾®å¤§ä¸€ç‚¹
    
    neg_mask = (y == 1)
    pos_mask = (y == 2)

    # é™ä½ alpha å’Œç‚¹å¤§å°ä»¥åº”å¯¹å¤§é‡æ•°æ®
    plt.scatter(X_2d[neg_mask, 0], X_2d[neg_mask, 1], c="blue", s=1, alpha=0.3, label="Negative")
    plt.scatter(X_2d[pos_mask, 0], X_2d[pos_mask, 1], c="red",  s=1, alpha=0.3, label="Positive")

    plt.legend(markerscale=5) # å›¾ä¾‹çš„ç‚¹æ”¾å¤§ä¸€ç‚¹æ–¹ä¾¿çœ‹
    plt.title(f"t-SNE of Amazon Review Vectors (n={X.shape[0]})")
    plt.tight_layout()

    out_path = os.path.join(base_dir, "tsne_docs_all_optimized.png")
    plt.savefig(out_path, dpi=300)
    print("âœ… è¯„è®ºçº§ t-SNE å›¾å·²ä¿å­˜åˆ°:", out_path)

if __name__ == "__main__":
    # Mac ä¸Š multiprocessing éœ€è¦è¿™ä¸ª
    multiprocessing.freeze_support()
    main()