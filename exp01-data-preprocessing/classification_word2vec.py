import os
import math
import re

import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
)
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# ç¡®ä¿åˆ†è¯èµ„æºå°±ç»ª
nltk.download("punkt")

# -------- 1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆå»ºè®®å’Œä½ å‰é¢ä¿æŒä¸€è‡´ï¼Œå¯ä»¥æŒ‰éœ€å†åŠ  stopwordsï¼‰ --------

def preprocess_text(text):
    """ç®€å•ç‰ˆæœ¬ï¼šå°å†™ã€å»æ ‡ç‚¹ã€åˆ†è¯"""
    if text is None:
        text = ""
    elif isinstance(text, float):
        if math.isnan(text):
            text = ""
        else:
            text = str(text)
    else:
        text = str(text)

    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    tokens = word_tokenize(text)

    # è¿™é‡Œå¯ä»¥æŒ‰éœ€è¦å¤åˆ¶ä½ åœ¨ wordcloud_sentiment.py é‡Œé‚£å¥—æ›´ä¸¥æ ¼çš„è¿‡æ»¤é€»è¾‘
    # æ¯”å¦‚åªä¿ç•™é•¿åº¦>2ã€å­—æ¯è¯ã€å»åœç”¨è¯ç­‰
    return tokens


# -------- 2. è¯»æ•°æ® + åˆå¹¶ title & review --------

def load_data(csv_name="test.csv"):
    base_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(base_dir, csv_name)

    df = pd.read_csv(csv_path)

    # ç¬¬0åˆ—ï¼šlabel (1=neg, 2=pos)ï¼Œç¬¬1åˆ—ï¼štitleï¼Œç¬¬2åˆ—ï¼šreview
    df["text"] = (
        df.iloc[:, 1].astype(str).fillna("") + " " +
        df.iloc[:, 2].astype(str).fillna("")
    )

    # æŠŠ label å˜æˆ 0 / 1 æ–¹ä¾¿ sklearn ä½¿ç”¨
    labels_raw = df.iloc[:, 0].values
    # Amazon polarity: 1 -> negative(0), 2 -> positive(1)
    y = np.array([0 if lab == 1 else 1 for lab in labels_raw], dtype=int)

    return df["text"].tolist(), y


# -------- 3. æŠŠä¸€æ¡è¯„è®ºå˜æˆâ€œå¹³å‡è¯å‘é‡â€ --------

def get_document_vector(tokens, model, vector_size=100):
    """å¯¹ä¸€æ¡è¯„è®ºçš„è¯å‘é‡å–å¹³å‡ï¼Œå¦‚æœå…¨æ˜¯ OOV è¯åˆ™è¿”å›é›¶å‘é‡"""
    vectors = []
    for w in tokens:
        if w in model.wv:
            vectors.append(model.wv[w])
    if not vectors:
        return np.zeros(vector_size, dtype=float)
    return np.mean(vectors, axis=0)


# -------- 4. ä¸»æµç¨‹ï¼šåŠ è½½æ¨¡å‹ -> æ„é€  X, y -> è®­ç»ƒåˆ†ç±»å™¨ -> ç”»æ··æ·†çŸ©é˜µ --------

def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))

    # 4.1 åŠ è½½ä½ ä¹‹å‰è®­ç»ƒå¥½çš„ Word2Vec æ¨¡å‹
    model_path = os.path.join(base_dir, "word2vec_sentiment.model")
    print(f"ğŸ‘‰ æ­£åœ¨åŠ è½½ Word2Vec æ¨¡å‹: {model_path}")
    model = Word2Vec.load(model_path)
    vector_size = model.vector_size
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‘é‡ç»´åº¦:", vector_size)

    # 4.2 åŠ è½½æ–‡æœ¬å’Œæ ‡ç­¾
    print("ğŸ‘‰ æ­£åœ¨åŠ è½½æ–‡æœ¬å’Œæ ‡ç­¾...")
    texts, y = load_data("test.csv")
    print("  æ ·æœ¬æ•°:", len(texts))

    # 4.3 æ–‡æœ¬é¢„å¤„ç† + æ–‡æ¡£å‘é‡
    print("ğŸ‘‰ æ­£åœ¨æ„é€ æ–‡æ¡£å‘é‡ X ...ï¼ˆå¯èƒ½ç¨å¾®æœ‰ç‚¹æ…¢ï¼‰")
    doc_vectors = []
    for i, text in enumerate(texts):
        tokens = preprocess_text(text)
        vec = get_document_vector(tokens, model, vector_size=vector_size)
        doc_vectors.append(vec)
        # å¯é€‰ï¼šçœ‹çœ‹è¿›åº¦
        # if (i + 1) % 5000 == 0:
        #     print(f"  å·²å¤„ç† {i+1} æ¡")

    X = np.array(doc_vectors)
    print("âœ… æ–‡æ¡£å‘é‡å½¢çŠ¶:", X.shape)
    print("âœ… æ ‡ç­¾å½¢çŠ¶:", y.shape)

    # 4.4 åˆ’åˆ†è®­ç»ƒé›† / æµ‹è¯•é›†
    print("ğŸ‘‰ åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print("  è®­ç»ƒé›†å¤§å°:", X_train.shape[0])
    print("  æµ‹è¯•é›†å¤§å°:", X_test.shape[0])

    # 4.5 ç‰¹å¾æ ‡å‡†åŒ–ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
    print("ğŸ‘‰ å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–(StandardScaler)...")
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # 4.6 è®­ç»ƒ Logistic Regressionï¼ˆæŒ‡å®šæ›´ç¨³å®šçš„ solver å¹¶å›ºå®šéšæœºç§å­ï¼‰
    print("ğŸ‘‰ è®­ç»ƒ Logistic Regression åˆ†ç±»å™¨...")
    clf = LogisticRegression(
        max_iter=1000,
        n_jobs=-1,
        solver="liblinear",
        random_state=42,
    )
    clf.fit(X_train, y_train)
    print("âœ… è®­ç»ƒå®Œæˆ")

    # 4.6 åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("ğŸ‘‰ åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"\nğŸ¯ æµ‹è¯•é›† Accuracy: {acc:.4f}\n")

    print("ğŸ‘‰ åˆ†ç±»æŠ¥å‘Š (precision / recall / f1)ï¼š")
    print(classification_report(
        y_test,
        y_pred,
        target_names=["negative", "positive"],
        digits=4,
    ))

    # 4.7 æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    print("ğŸ‘‰ æ··æ·†çŸ©é˜µåŸå§‹æ•°å€¼ï¼š")
    print(cm)

    # 4.8 ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
    plt.figure(figsize=(5, 4))
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=["Pred Neg", "Pred Pos"],
        yticklabels=["True Neg", "True Pos"],
    )
    plt.title("Confusion Matrix (Logistic Regression on Word2Vec doc vectors)")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    plt.tight_layout()

    out_path = os.path.join(base_dir, "confusion_matrix_word2vec.png")
    plt.savefig(out_path, dpi=300)
    print(f"âœ… æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {out_path}")
    plt.show()


if __name__ == "__main__":
    main()