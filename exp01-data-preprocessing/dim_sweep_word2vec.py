import os
import math
import re
import time

import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

import matplotlib.pyplot as plt
import matplotlib

# ç¡®ä¿åˆ†è¯èµ„æºå°±ç»ª
nltk.download("punkt")

# -------- 0. ï¼ˆå¯é€‰ï¼‰ä¸­æ–‡å­—ä½“ï¼Œæ–¹ä¾¿ç”»å›¾ä¸­æ–‡æ ‡é¢˜ --------
def set_chinese_font():
    import matplotlib.font_manager
    font_list = ['PingFang SC', 'Heiti SC', 'STHeiti', 'SimHei', 'Microsoft YaHei', 'SimSun']
    available_fonts = set(f.name for f in matplotlib.font_manager.fontManager.ttflist)
    for font in font_list:
        if font in available_fonts:
            matplotlib.rcParams['font.sans-serif'] = [font]
            matplotlib.rcParams['axes.unicode_minus'] = False
            print(f"âœ… å·²å¯ç”¨ä¸­æ–‡å­—ä½“: {font}")
            return
    print("âš ï¸ æœªæ£€æµ‹åˆ°å¸¸ç”¨ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­æ–‡å¯èƒ½ä¼šæ˜¾ç¤ºä¹±ç ã€‚")


# -------- 1. æ–‡æœ¬é¢„å¤„ç†ï¼ˆå’Œ classification_wordvec2.py åŸºæœ¬ä¸€è‡´ï¼‰ --------
def preprocess_text(text):
    if text is None:
        text = ""
    elif isinstance(text, float):
        text = "" if math.isnan(text) else str(text)
    else:
        text = str(text)
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    tokens = word_tokenize(text)
    return tokens


# -------- 2. è¯»æ•°æ® --------
def load_data(csv_name="test.csv", max_samples=None):
    """
    max_samples: ä¸ºäº†åŠ å¿«å®éªŒï¼Œå¯ä»¥åªå–å‰ max_samples æ¡æ¥åšç»´åº¦å¯¹æ¯”
    """
    base_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(base_dir, csv_name)
    df = pd.read_csv(csv_path)

    # åˆå¹¶ title + review
    df["text"] = (
        df.iloc[:, 1].astype(str).fillna("") + " " +
        df.iloc[:, 2].astype(str).fillna("")
    )

    if max_samples is not None and len(df) > max_samples:
        df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)

    labels_raw = df.iloc[:, 0].values
    y = np.array([0 if lab == 1 else 1 for lab in labels_raw], dtype=int)
    texts = df["text"].tolist()
    return texts, y


# -------- 3. æ–‡æ¡£å‘é‡ï¼šå¹³å‡è¯å‘é‡ --------
def get_document_vector(tokens, model, vector_size):
    vectors = []
    for w in tokens:
        if w in model.wv:
            vectors.append(model.wv[w])
    if not vectors:
        return np.zeros(vector_size, dtype=float)
    return np.mean(vectors, axis=0)


def main():
    set_chinese_font()
    base_dir = os.path.dirname(os.path.abspath(__file__))

    # 1. åŠ è½½æ•°æ®ï¼ˆå¯ä»¥å…ˆç”¨è¾ƒå°‘æ ·æœ¬å¿«é€Ÿå¯¹æ¯”ï¼Œæ¯”å¦‚ 100000ï¼Œå¦‚æœä½ æƒ³å…¨é‡å°±æ”¹æˆ Noneï¼‰
    print("ğŸ‘‰ åŠ è½½æ•°æ®ï¼Œç”¨äºç»´åº¦å¯¹æ¯”å®éªŒ...")
    texts, y = load_data("test.csv", max_samples=100000)
    print("  æ ·æœ¬æ•°:", len(texts))

    # 2. å…ˆæŠŠæ–‡æœ¬é¢„å¤„ç† + åˆ†å¥½è¯ï¼Œæ–¹ä¾¿é‡å¤åˆ©ç”¨
    print("ğŸ‘‰ æ–‡æœ¬é¢„å¤„ç† & åˆ†è¯...")
    corpus = [preprocess_text(t) for t in texts]

    # 3. é¢„å…ˆåˆ’åˆ†å¥½ train/test ç´¢å¼•ï¼Œä¿è¯ä¸åŒç»´åº¦å®éªŒå¯æ¯”
    print("ğŸ‘‰ åˆ’åˆ†ç»Ÿä¸€çš„è®­ç»ƒ/æµ‹è¯•ç´¢å¼•...")
    indices = np.arange(len(y))
    idx_train, idx_test, y_train, y_test = train_test_split(
        indices, y, test_size=0.2, random_state=42, stratify=y
    )

    dims = [50, 100, 200, 300]
    results = []

    for dim in dims:
        print("\n" + "=" * 60)
        print(f"ğŸ”¢ æ­£åœ¨å®éªŒå‘é‡ç»´åº¦ dim = {dim}")
        start_time = time.time()

        # 3.1 è®­ç»ƒ Word2Vec
        print("ğŸ‘‰ è®­ç»ƒ Word2Vec æ¨¡å‹...")
        model = Word2Vec(
            sentences=corpus,
            vector_size=dim,
            window=5,
            min_count=1,
            workers=4,    # è¿™é‡Œå¯ä»¥å¤šçº¿ç¨‹åŠ é€Ÿ
            sg=1,         # 1=skip-gram, 0=CBOWï¼Œä½ å¯ä»¥æŒ‰éœ€è¦æ”¹
        )

        # 3.2 æ„é€ æ–‡æ¡£å‘é‡çŸ©é˜µ X
        print("ğŸ‘‰ æ„é€ æ–‡æ¡£å‘é‡...")
        doc_vectors = [
            get_document_vector(tokens, model, vector_size=dim)
            for tokens in corpus
        ]
        X = np.array(doc_vectors)

        # 3.3 æŒ‰ç»Ÿä¸€ç´¢å¼•åˆ’åˆ† train/test
        X_train = X[idx_train]
        X_test = X[idx_test]

        # 3.4 æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_std = scaler.fit_transform(X_train)
        X_test_std = scaler.transform(X_test)

        # 3.5 è®­ç»ƒ Logistic å›å½’
        print("ğŸ‘‰ è®­ç»ƒ Logistic Regression...")
        clf = LogisticRegression(
            max_iter=1000,
            solver="liblinear",
            random_state=42,
        )
        clf.fit(X_train_std, y_train)

        # 3.6 è¯„ä¼°
        y_pred = clf.predict(X_test_std)
        acc = accuracy_score(y_test, y_pred)
        elapsed = time.time() - start_time
        print(f"ğŸ¯ dim = {dim} çš„æµ‹è¯•é›† Accuracy: {acc:.4f}ï¼Œè€—æ—¶çº¦ {elapsed:.1f} ç§’")

        results.append((dim, acc, elapsed))

    # 4. æ‰“å°ç»“æœè¡¨
    print("\n" + "=" * 60)
    print("ç»´åº¦ vs å‡†ç¡®ç‡ å¯¹æ¯”ç»“æœï¼š")
    for dim, acc, elapsed in results:
        print(f"  dim = {dim:3d}  ->  Accuracy = {acc:.4f}ï¼Œè€—æ—¶çº¦ {elapsed:.1f} ç§’")

    # 5. ç”»ç»´åº¦-å‡†ç¡®ç‡æŠ˜çº¿å›¾
    dims_list = [r[0] for r in results]
    acc_list = [r[1] for r in results]

    plt.figure(figsize=(7, 5), dpi=120)
    plt.plot(dims_list, acc_list, marker="o")
    plt.title("è¯å‘é‡ç»´åº¦ vs åˆ†ç±»å‡†ç¡®ç‡", fontsize=14)
    plt.xlabel("è¯å‘é‡ç»´åº¦ (vector_size)", fontsize=12)
    plt.ylabel("æµ‹è¯•é›† Accuracy", fontsize=12)
    plt.grid(True)
    plt.tight_layout()

    out_path = os.path.join(base_dir, "dim_vs_accuracy.png")
    plt.savefig(out_path, dpi=300)
    print(f"âœ… ç»´åº¦-å‡†ç¡®ç‡æŠ˜çº¿å›¾å·²ä¿å­˜åˆ°: {out_path}")
    plt.show()


if __name__ == "__main__":
    main()