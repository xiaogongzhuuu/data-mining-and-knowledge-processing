# RAG 系统有效性检测方案

## 一、问题分析总结

### 1.1 发现的问题
- **显示问题**：检索结果显示"全是同一个作者（邵长荣）"
- **根本原因**：数据中的 `title` 字段只包含作者名字，而不是文档具体标题
- **实际状况**：数据中包含 9 位作者的 355 篇文档，检索结果实际上是多样化的

### 1.2 数据分布统计
根据对 `processed_data.json` 的分析，数据源包含以下作者：
- 吴银根.txt: 约 42 篇
- 唐汉钧.txt: 约 56 篇
- 徐振晔.txt: 约 44 篇
- 徐蓉娟.txt: 约 42 篇
- 施杞.txt: 约 56 篇
- 王大增.txt: 约 48 篇
- 王育群.txt: 约 41 篇
- 邱佳信.txt: 约 42 篇
- 邵长荣.txt: 约 40 篇

**总计**：355 篇文档

### 1.3 已完成的修复
✅ 修改了 `app.py` 的显示逻辑，现在会显示：
- 文档 ID（从 id_to_doc_map 中获取）
- 标题
- 来源文件
- 摘要
- 相似度分数

---

## 二、RAG 有效性检测方案

### 2.1 检测维度

#### 2.1.1 检索质量检测
**目标**：评估检索系统的准确性和多样性

**指标**：
1. **检索相关性（Relevance）**
   - 定义：检索到的文档是否包含查询答案
   - 测量方法：人工评估 + 关键词匹配
   - 评分标准：0-1（0=不相关，1=完全相关）

2. **检索多样性（Diversity）**
   - 定义：检索结果是否包含不同作者、不同主题的文档
   - 测量方法：统计检索结果中不同作者的数量
   - 评分标准：不同作者数量 / TOP_K

3. **检索排名质量（Ranking）**
   - 定义：最相关的文档是否排在前面
   - 测量方法：人工评估前 3 个文档的相关性
   - 评分标准：0-1

#### 2.1.2 生成质量检测
**目标**：评估生成答案的准确性和完整性

**指标**：
1. **语义准确性（Semantic Accuracy）**
   - 定义：答案的意思是否正确
   - 测量方法：人工评估
   - 评分标准：0-1

2. **专业术语匹配（Terminology Match）**
   - 定义：是否使用正确的医学术语
   - 测量方法：关键词匹配
   - 评分标准：匹配关键词数 / 期望关键词数

3. **完整性（Completeness）**
   - 定义：答案是否完整回答了用户问题
   - 测量方法：人工评估
   - 评分标准：0-1

4. **幻觉检测（Hallucination Detection）**
   - 定义：答案是否包含文档中没有的信息
   - 测量方法：人工检查
   - 评分标准：0-1（0=有幻觉，1=无幻觉）

#### 2.1.3 性能检测
**目标**：评估系统的响应速度和稳定性

**指标**：
1. **响应时间（Response Time）**
   - 定义：从输入查询到显示答案的时间
   - 测量方法：自动计时
   - 目标：< 15 秒

2. **系统稳定性（Stability）**
   - 定义：多次查询结果是否一致
   - 测量方法：重复查询测试
   - 评分标准：一致性分数

---

### 2.2 检测方法

#### 2.2.1 自动化测试
**工具**：`test_rag_performance.py`

**测试用例**：
```python
TEST_QUERIES = [
    "吴银根的学术思想是什么？",
    "施杞在中医外科方面有什么贡献？",
    "如何治疗慢性阻塞性肺疾病？",
    "中医调理气血的方法有哪些？",
    "肺肾两脏的关系是什么？",
    "如何理解'以平为期'的治疗原则？"
]
```

**评估方法**：
- 关键词匹配（自动）
- 响应时间测量（自动）
- 生成质量评估（半自动）

#### 2.2.2 人工评估
**评估流程**：

1. **准备阶段**
   - 选择 10-20 个测试查询
   - 为每个查询准备期望的关键词和答案要点
   - 准备评估表格

2. **执行阶段**
   - 对每个查询执行 RAG 检索
   - 记录检索结果（文档 ID、相似度、来源）
   - 记录生成的答案
   - 人工评估检索相关性和生成质量

3. **评分阶段**
   - 使用评分标准对每个维度打分
   - 计算平均分和标准差
   - 分析问题和改进方向

**评估表格模板**：

| 查询 | 检索相关性 | 检索多样性 | 语义准确性 | 专业术语 | 完整性 | 幻觉检测 | 响应时间 | 备注 |
|------|-----------|-----------|-----------|---------|-------|---------|---------|------|
| Q1   | 0.8       | 0.6       | 0.9       | 0.8     | 0.7   | 1.0     | 12.3s   | -    |
| Q2   | 0.7       | 0.8       | 0.8       | 0.7     | 0.8   | 1.0     | 10.5s   | -    |
| ...  | ...       | ...       | ...       | ...     | ...   | ...     | ...     | -    |

#### 2.2.3 A/B 测试
**目的**：比较不同配置的效果

**测试变量**：
1. 嵌入模型：m3e-base vs 其他模型
2. 检索策略：简单检索 vs 混合检索
3. TOP_K 值：3 vs 5 vs 10
4. 重排序权重：不同权重组合

**测试方法**：
- 保持其他变量不变，只改变一个变量
- 使用相同的测试查询
- 比较性能指标
- 选择最优配置

---

### 2.3 检测流程

#### 阶段 1：自动化测试（1-2 小时）
1. 运行 `test_rag_performance.py`
2. 收集自动化测试结果
3. 分析自动测试的局限性

#### 阶段 2：人工评估（2-4 小时）
1. 准备测试查询和评估表格
2. 执行人工评估
3. 记录评估结果
4. 分析评估数据

#### 阶段 3：问题定位（1-2 小时）
1. 分析检索失败的原因
2. 分析生成质量差的原因
3. 识别系统性问题

#### 阶段 4：优化改进（根据发现的问题）
1. 调整检索参数
2. 优化提示词
3. 改进数据分块
4. 重新测试

#### 阶段 5：回归测试（1 小时）
1. 运行完整的测试套件
2. 验证改进效果
3. 确保没有引入新问题

---

### 2.4 评分标准

#### 优秀（⭐⭐⭐）
- 检索相关性 ≥ 0.8
- 检索多样性 ≥ 0.7
- 语义准确性 ≥ 0.8
- 专业术语匹配 ≥ 0.8
- 完整性 ≥ 0.8
- 无幻觉
- 响应时间 < 10 秒

#### 良好（⭐⭐）
- 检索相关性 ≥ 0.7
- 检索多样性 ≥ 0.6
- 语义准确性 ≥ 0.7
- 专业术语匹配 ≥ 0.7
- 完整性 ≥ 0.7
- 偶有轻微幻觉
- 响应时间 < 15 秒

#### 中等（⭐）
- 检索相关性 ≥ 0.6
- 检索多样性 ≥ 0.5
- 语义准确性 ≥ 0.6
- 专业术语匹配 ≥ 0.6
- 完整性 ≥ 0.6
- 有轻微幻觉
- 响应时间 < 20 秒

#### 需要改进（⚠️）
- 任何指标低于中等标准

---

### 2.5 检测报告模板

```markdown
# RAG 系统有效性检测报告

## 检测概览
- 检测日期：YYYY-MM-DD
- 检测人员：XXX
- 系统版本：v2.0
- 测试查询数量：N 个

## 自动化测试结果

### 性能指标
- 平均检索相关性：XX%
- 平均生成质量：XX%
- 平均响应时间：XX 秒

### 测试用例详情
[列出每个测试用例的详细结果]

## 人工评估结果

### 检索质量
- 平均检索相关性：XX%
- 平均检索多样性：XX%
- 平均检索排名质量：XX%

### 生成质量
- 平均语义准确性：XX%
- 平均专业术语匹配：XX%
- 平均完整性：XX%
- 幻觉检测：XX%

### 性能指标
- 平均响应时间：XX 秒
- 系统稳定性：XX%

## 问题分析

### 发现的问题
1. [问题描述]
   - 影响：[影响程度]
   - 原因：[根本原因]
   - 解决方案：[建议方案]

2. [问题描述]
   - ...

## 改进建议

### 短期改进（1周内）
1. [改进项]
2. [改进项]

### 中期改进（1个月内）
1. [改进项]
2. [改进项]

### 长期改进（3个月内）
1. [改进项]
2. [改进项]

## 总体评价
[总体评价和结论]

## 附录
- 测试查询列表
- 评估表格
- 详细日志
```

---

## 三、执行建议

### 3.1 立即执行
1. ✅ 修复显示问题（已完成）
2. 运行自动化性能测试
3. 分析自动化测试结果

### 3.2 本周完成
1. 执行人工评估
2. 生成检测报告
3. 根据报告进行优化

### 3.3 持续改进
1. 定期运行性能测试
2. 收集用户反馈
3. 持续优化系统

---

## 四、工具和资源

### 4.1 测试工具
- `test_rag_performance.py` - 自动化性能测试
- `diagnostics.py` - 系统诊断
- `test_models.py` - 模型测试

### 4.2 评估工具
- 人工评估表格（Excel/Google Sheets）
- 评分标准文档
- 问题追踪系统

### 4.3 参考文档
- `OPTIMIZATION_SUMMARY.md` - 优化总结
- `TROUBLESHOOTING.md` - 问题排查
- `DIAGNOSTICS.md` - 检测方案

---

**最后更新**：2026-01-07
**版本**：v1.0