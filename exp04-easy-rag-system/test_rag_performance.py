"""
RAG ç³»ç»Ÿæ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ç»´åº¦ï¼š
1. æ£€ç´¢ç›¸å…³æ€§
2. ç”Ÿæˆè´¨é‡ï¼ˆè¯­ä¹‰å‡†ç¡®æ€§ã€ä¸“ä¸šæœ¯è¯­åŒ¹é…ï¼‰
3. å“åº”æ—¶é—´
"""

import time
import json
import sys
import os

# æ·»åŠ ç¯å¢ƒå˜é‡
os.environ['HF_HOME'] = './hf_cache'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

from config import (
    DATA_FILE, EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, TOP_K,
    MAX_ARTICLES_TO_INDEX, COLLECTION_NAME, OLLAMA_MODEL, OLLAMA_BASE_URL,
    EMBEDDING_DIM
)
from data_utils import load_data
from models import load_embedding_model, load_generation_model
from chroma_utils import get_chroma_client, setup_chroma_collection, index_data_if_needed, search_similar_documents, id_to_doc_map
from rag_core import generate_answer
from retrieval_optimizer import hybrid_search, rerank_documents, remove_duplicate_documents

# æµ‹è¯•æŸ¥è¯¢é›†
TEST_QUERIES = [
    {
        "query": "å´é“¶æ ¹çš„å­¦æœ¯æ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ",
        "expected_keywords": ["æ°”è¡€é˜´é˜³", "å¹³", "åŠ¨æ€", "ç›¸å¯¹"],
        "description": "æµ‹è¯•å¯¹å´é“¶æ ¹å­¦æœ¯æ€æƒ³çš„æ£€ç´¢"
    },
    {
        "query": "æ–½æåœ¨ä¸­åŒ»å¤–ç§‘æ–¹é¢æœ‰ä»€ä¹ˆè´¡çŒ®ï¼Ÿ",
        "expected_keywords": ["ä¸­åŒ»å¤–ç§‘", "ä¸´åºŠ", "ç»éªŒ"],
        "description": "æµ‹è¯•å¯¹æ–½æä¸“ä¸šé¢†åŸŸçš„æ£€ç´¢"
    },
    {
        "query": "å¦‚ä½•æ²»ç–—æ…¢æ€§é˜»å¡æ€§è‚ºç–¾ç—…ï¼Ÿ",
        "expected_keywords": ["è‚ºç—…", "æ²»ç–—", "è¾¨è¯"],
        "description": "æµ‹è¯•å¯¹ç–¾ç—…æ²»ç–—æ–¹æ¡ˆçš„æ£€ç´¢"
    },
    {
        "query": "ä¸­åŒ»è°ƒç†æ°”è¡€çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ",
        "expected_keywords": ["æ°”è¡€", "è°ƒç†", "æ–¹è¯"],
        "description": "æµ‹è¯•å¯¹ä¸­åŒ»è°ƒç†æ–¹æ³•çš„æ£€ç´¢"
    },
    {
        "query": "è‚ºè‚¾ä¸¤è„çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
        "expected_keywords": ["è‚º", "è‚¾", "æ¯å­", "ç›¸ç”Ÿ"],
        "description": "æµ‹è¯•å¯¹è„è…‘å…³ç³»çš„æ£€ç´¢"
    },
    {
        "query": "å¦‚ä½•ç†è§£'ä»¥å¹³ä¸ºæœŸ'çš„æ²»ç–—åŸåˆ™ï¼Ÿ",
        "expected_keywords": ["å¹³", "è°ƒå’Œ", "é˜´é˜³", "æ°”è¡€"],
        "description": "æµ‹è¯•å¯¹æ²»ç–—åŸåˆ™çš„ç†è§£"
    }
]

def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"  {title}")
    print("="*60 + "\n")

def test_retrieval_relevance(query, retrieved_docs, expected_keywords):
    """æµ‹è¯•æ£€ç´¢ç›¸å…³æ€§"""
    if not retrieved_docs:
        return 0.0, "æœªæ£€ç´¢åˆ°æ–‡æ¡£"

    # æ£€æŸ¥æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­æ˜¯å¦åŒ…å«æœŸæœ›çš„å…³é”®è¯
    keyword_hits = 0
    for doc in retrieved_docs:
        content = doc.get('content', '').lower()
        for keyword in expected_keywords:
            if keyword.lower() in content:
                keyword_hits += 1
                break

    relevance_score = min(keyword_hits / len(expected_keywords), 1.0)
    return relevance_score, f"å‘½ä¸­ {keyword_hits}/{len(expected_keywords)} ä¸ªå…³é”®è¯"

def test_generation_quality(answer, expected_keywords):
    """æµ‹è¯•ç”Ÿæˆè´¨é‡"""
    if not answer:
        return 0.0, "æœªç”Ÿæˆç­”æ¡ˆ"

    # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å«æœŸæœ›çš„å…³é”®è¯
    keyword_hits = 0
    answer_lower = answer.lower()
    for keyword in expected_keywords:
        if keyword.lower() in answer_lower:
            keyword_hits += 1

    quality_score = min(keyword_hits / len(expected_keywords), 1.0)
    return quality_score, f"ç­”æ¡ˆåŒ…å« {keyword_hits}/{len(expected_keywords)} ä¸ªå…³é”®è¯"

def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print_section("RAG ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")

    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("1. åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
    start_time = time.time()

    chroma_client = get_chroma_client()
    if not chroma_client:
        print("âŒ ChromaDB å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
        return

    collection_ready = setup_chroma_collection(chroma_client)
    if not collection_ready:
        print("âŒ ChromaDB Collection è®¾ç½®å¤±è´¥")
        return

    embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
    if not embedding_model:
        print("âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    generation_model, tokenizer = load_generation_model(GENERATION_MODEL_NAME)
    if not generation_model:
        print("âŒ ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥")
        return

    init_time = time.time() - start_time
    print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f} ç§’\n")

    # 2. åŠ è½½å’Œç´¢å¼•æ•°æ®
    print("2. åŠ è½½å’Œç´¢å¼•æ•°æ®...")
    start_time = time.time()

    pubmed_data = load_data(DATA_FILE)
    if not pubmed_data:
        print(f"âŒ æ— æ³•ä» {DATA_FILE} åŠ è½½æ•°æ®")
        return

    print(f"   å·²åŠ è½½ {len(pubmed_data)} æ¡æ•°æ®")

    indexing_successful = index_data_if_needed(chroma_client, pubmed_data, embedding_model)
    if not indexing_successful:
        print("âŒ æ•°æ®ç´¢å¼•å¤±è´¥")
        return

    index_time = time.time() - start_time
    print(f"âœ… æ•°æ®ç´¢å¼•å®Œæˆï¼Œè€—æ—¶: {index_time:.2f} ç§’\n")

    # 3. è¿è¡Œæµ‹è¯•æŸ¥è¯¢
    print_section("3. è¿è¡Œæµ‹è¯•æŸ¥è¯¢")

    total_retrieval_score = 0.0
    total_generation_score = 0.0
    total_response_time = 0.0

    for i, test_case in enumerate(TEST_QUERIES, 1):
        query = test_case["query"]
        expected_keywords = test_case["expected_keywords"]
        description = test_case["description"]

        print(f"\næµ‹è¯• {i}/{len(TEST_QUERIES)}: {description}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"æœŸæœ›å…³é”®è¯: {', '.join(expected_keywords)}")

        # æ‰§è¡ŒæŸ¥è¯¢
        start_time = time.time()

        # ä½¿ç”¨æ··åˆæ£€ç´¢
        retrieved_docs, distances = hybrid_search(chroma_client, query, embedding_model, top_k=TOP_K)

        if not retrieved_docs:
            print("âŒ æ£€ç´¢å¤±è´¥ï¼šæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            continue

        # å»é‡
        retrieved_docs = remove_duplicate_documents(retrieved_docs)

        if not retrieved_docs:
            print("âŒ æ£€ç´¢å¤±è´¥ï¼šå»é‡åæ— æ–‡æ¡£")
            continue

        # ç”Ÿæˆç­”æ¡ˆ
        answer = generate_answer(query, retrieved_docs, generation_model, tokenizer)

        response_time = time.time() - start_time
        total_response_time += response_time

        # è¯„ä¼°
        retrieval_score, retrieval_detail = test_retrieval_relevance(query, retrieved_docs, expected_keywords)
        generation_score, generation_detail = test_generation_quality(answer, expected_keywords)

        total_retrieval_score += retrieval_score
        total_generation_score += generation_score

        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š æ£€ç´¢ç»“æœ:")
        print(f"   - æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªæ–‡æ¡£")
        print(f"   - ç›¸å…³æ€§è¯„åˆ†: {retrieval_score:.2f} ({retrieval_detail})")

        print(f"\nğŸ“ ç”Ÿæˆç»“æœ:")
        print(f"   - ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
        print(f"   - è´¨é‡è¯„åˆ†: {generation_score:.2f} ({generation_detail})")
        print(f"   - å“åº”æ—¶é—´: {response_time:.2f} ç§’")

        print(f"\nğŸ’¬ ç”Ÿæˆçš„ç­”æ¡ˆ:")
        print(f"   {answer[:300]}...")

    # 4. æ±‡æ€»ç»“æœ
    print_section("4. æ€§èƒ½æµ‹è¯•æ±‡æ€»")

    avg_retrieval_score = total_retrieval_score / len(TEST_QUERIES)
    avg_generation_score = total_generation_score / len(TEST_QUERIES)
    avg_response_time = total_response_time / len(TEST_QUERIES)

    print(f"âœ… æµ‹è¯•å®Œæˆï¼å…±æµ‹è¯• {len(TEST_QUERIES)} ä¸ªæŸ¥è¯¢\n")
    print(f"å¹³å‡æ£€ç´¢ç›¸å…³æ€§: {avg_retrieval_score:.2%}")
    print(f"å¹³å‡ç”Ÿæˆè´¨é‡: {avg_generation_score:.2%}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f} ç§’")
    print(f"ç³»ç»Ÿåˆå§‹åŒ–æ—¶é—´: {init_time:.2f} ç§’")
    print(f"æ•°æ®ç´¢å¼•æ—¶é—´: {index_time:.2f} ç§’")

    # æ€§èƒ½è¯„çº§
    print("\nğŸ“ˆ æ€§èƒ½è¯„çº§:")
    if avg_retrieval_score >= 0.8 and avg_generation_score >= 0.8:
        print("   â­â­â­ ä¼˜ç§€ - ç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼Œæ£€ç´¢å’Œç”Ÿæˆè´¨é‡å‡è¾¾åˆ°é«˜æ ‡å‡†")
    elif avg_retrieval_score >= 0.7 and avg_generation_score >= 0.7:
        print("   â­â­ è‰¯å¥½ - ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œå„é¡¹æŒ‡æ ‡è¾¾åˆ°é¢„æœŸ")
    elif avg_retrieval_score >= 0.6 and avg_generation_score >= 0.6:
        print("   â­ ä¸­ç­‰ - ç³»ç»Ÿè¡¨ç°å°šå¯ï¼Œä»æœ‰ä¼˜åŒ–ç©ºé—´")
    else:
        print("   âš ï¸ éœ€è¦æ”¹è¿› - å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆç­–ç•¥")

    # ä¼˜åŒ–å»ºè®®
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    if avg_retrieval_score < 0.7:
        print("   - æ£€ç´¢ç›¸å…³æ€§è¾ƒä½ï¼Œå»ºè®®ï¼š")
        print("     * å¢åŠ æ£€ç´¢çš„æ–‡æ¡£æ•°é‡ï¼ˆTOP_Kï¼‰")
        print("     * ä¼˜åŒ–åµŒå…¥æ¨¡å‹æˆ–å°è¯•å…¶ä»–æ¨¡å‹")
        print("     * æ”¹è¿›æ•°æ®åˆ†å—ç­–ç•¥")
    
    if avg_generation_score < 0.7:
        print("   - ç”Ÿæˆè´¨é‡è¾ƒä½ï¼Œå»ºè®®ï¼š")
        print("     * è¿›ä¸€æ­¥ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹")
        print("     * å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶")
        print("     * è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆtemperature, top_pï¼‰")
    
    if avg_response_time > 10:
        print("   - å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ï¼š")
        print("     * ä¼˜åŒ–æ¨¡å‹åŠ è½½å’Œç¼“å­˜")
        print("     * è€ƒè™‘ä½¿ç”¨æ›´è½»é‡çš„æ¨¡å‹")
        print("     * ä¼˜åŒ–æ£€ç´¢ç®—æ³•æ•ˆç‡")

if __name__ == "__main__":
    try:
        run_performance_test()
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()