"""
è®­ç»ƒè„šæœ¬ï¼šå¯¹ Qwen2.5-0.5B åšæƒ…æ„Ÿåˆ†ç±»å¾®è°ƒï¼ˆé€‚é… 8GB GPUï¼‰
- è‡ªåŠ¨è¯»å–å½“å‰ç›®å½•ä¸‹çš„ train.csv, dev.csv, test.csv
- ä½¿ç”¨ Hugging Face tokenizer + AutoModel (trust_remote_code=True)
- è‡ªå®šä¹‰åˆ†ç±»å¤´ + è®­ç»ƒå¾ªç¯
"""

import os
import csv
import math
import random
from typing import List, Tuple

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix
import json

# ----------------- é…ç½®é¡¹ -----------------
MODEL_NAME = "Qwen/Qwen2.5-0.5B"
CACHE_DIR = "../hf_cache"  # ä½¿ç”¨æœ¬åœ°ç¼“å­˜ç›®å½•
OUTPUT_DIR = "./qwen_sentiment_out"   # è¾“å‡ºç›®å½•
LOG_DIR = "./logs"                     # æ—¥å¿—ç›®å½•

MAX_LENGTH = 128                      # æ–‡æœ¬æˆªæ–­é•¿åº¦ï¼ˆæ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼š48/128/256ï¼‰
MAX_TRAIN_SAMPLES = 2000              # è®­ç»ƒé›†æœ€å¤§æ ·æœ¬æ•°ï¼ˆå¿«é€Ÿå®éªŒå¯é™åˆ° 500-1000ï¼‰

per_device_batch_size = 4             # æ¯å¡ batchï¼ˆæ˜¾å­˜ä¸è¶³é™åˆ° 1-2ï¼‰
gradient_accumulation_steps = 4        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
num_epochs = 3                        # è®­ç»ƒè½®æ•°
learning_rate = 3e-5                  # å­¦ä¹ ç‡
weight_decay = 0.01                   # æƒé‡è¡°å‡
seed = 42                             # éšæœºç§å­
# ------------------------------------------

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸  Using device: {device}")


# å›ºå®šéšæœºç§å­
def set_seed(s=seed):
    random.seed(s)
    torch.manual_seed(s)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(s)


set_seed()


# --------- æ•°æ®åŠ è½½ ---------
def load_csv_data(filepath: str) -> Tuple[List[str], List[int]]:
    """
    åŠ è½½ CSV æ ¼å¼çš„æ•°æ®æ–‡ä»¶
    æ ¼å¼ï¼šlabel,text
    è¿”å›ï¼štexts, labels
    """
    texts, labels = [], []
    
    if not os.path.exists(filepath):
        print(f"âš ï¸  Warning: {filepath} not found")
        return texts, labels
    
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        reader = csv.reader(f)
        next(reader, None)  # è·³è¿‡è¡¨å¤´ï¼ˆå¦‚æœæœ‰ï¼‰
        
        for row in reader:
            if len(row) < 2:
                continue
            
            try:
                label = int(row[0].strip())
                text = row[1].strip() if len(row) > 1 else ""
                
                # åˆå¹¶å¤šåˆ—æ–‡æœ¬ï¼ˆå¤„ç†æ–‡æœ¬ä¸­åŒ…å«é€—å·çš„æƒ…å†µï¼‰
                if len(row) > 2:
                    text = " ".join(r.strip() for r in row[1:] if r.strip())
                
                text = text.replace("\n", " ").replace("\r", " ").strip()
                
                if not text:
                    continue
                
                # ç»Ÿä¸€ label åˆ° 0/1ï¼ˆå‡è®¾ 1=è´Ÿé¢, 2=æ­£é¢ï¼‰
                mapped_label = 0 if label == 1 else 1
                
                texts.append(text)
                labels.append(mapped_label)
                
            except (ValueError, IndexError):
                continue
    
    return texts, labels


def load_dataset():
    """åŠ è½½è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®"""
    print("ğŸ“‚ Loading datasets...")
    
    train_texts, train_labels = load_csv_data("../train.csv")
    dev_texts, dev_labels = load_csv_data("../dev.csv")
    test_texts, test_labels = load_csv_data("../test.csv")
    
    if len(train_texts) == 0:
        raise RuntimeError("âŒ No training data found. Check train.csv")
    
    # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä»è®­ç»ƒé›†åˆ‡åˆ†
    if len(dev_texts) == 0 and len(train_texts) >= 20:
        n_dev = max(int(0.1 * len(train_texts)), 20)
        dev_texts = train_texts[:n_dev]
        dev_labels = train_labels[:n_dev]
        train_texts = train_texts[n_dev:]
        train_labels = train_labels[n_dev:]
        print(f"ğŸ“Š Split {n_dev} samples from train as dev set")
    
    # éšæœºä¸‹é‡‡æ ·è®­ç»ƒé›†ï¼ˆæ ¸å¿ƒåŠ é€Ÿç‚¹ï¼‰
    if len(train_texts) > MAX_TRAIN_SAMPLES:
        rng = random.Random(42)
        indices = list(range(len(train_texts)))
        rng.shuffle(indices)
        indices = indices[:MAX_TRAIN_SAMPLES]
        
        train_texts = [train_texts[i] for i in indices]
        train_labels = [train_labels[i] for i in indices]
        
        print(f"ğŸš€ Randomly sampled train set to {MAX_TRAIN_SAMPLES}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    def label_stats(labels):
        if not labels:
            return {"total": 0, "pos": 0, "neg": 0}
        total = len(labels)
        pos = sum(labels)
        return {"total": total, "pos": pos, "neg": total - pos}
    
    print("\nğŸ“Š Dataset Statistics:")
    print(f"  Train: {label_stats(train_labels)}")
    print(f"  Dev  : {label_stats(dev_labels)}")
    print(f"  Test : {label_stats(test_labels)}")
    print()
    
    return (
        train_texts, train_labels,
        dev_texts, dev_labels,
        test_texts, test_labels
    )


# --------- Dataset & DataLoader ---------
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        
        item = {k: v.squeeze(0) for k, v in encoding.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item


def make_dataloader(texts, labels, tokenizer, batch_size, shuffle=True):
    if not texts:
        return None
    dataset = SentimentDataset(texts, labels, tokenizer, max_length=MAX_LENGTH)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


# --------- Model: Qwen + Classification Head ---------
class QwenForSequenceClassification(nn.Module):
    def __init__(self, model_name, num_labels=2, cache_dir=None):
        super().__init__()
        
        print(f"ğŸ”„ Loading {model_name}...")
        self.base = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=cache_dir,
            low_cpu_mem_usage=True
        )
        
        # è·å–éšè—å±‚å¤§å°
        hidden_size = None
        cfg = getattr(self.base, "config", None)
        if cfg:
            hidden_size = getattr(cfg, "hidden_size", None) or getattr(cfg, "d_model", None)
        
        self.num_labels = num_labels
        self.classifier = None
        
        if hidden_size is not None:
            self.classifier = nn.Linear(hidden_size, num_labels)
            print(f"âœ… Model loaded (hidden_size={hidden_size})")
    
    def forward(self, input_ids, attention_mask, labels=None):
        # è·å– base model è¾“å‡º
        outputs = self.base(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=False,
            return_dict=True
        )
        
        # æå–æœ€åä¸€å±‚éšè—çŠ¶æ€
        if hasattr(outputs, "last_hidden_state"):
            last_hidden = outputs.last_hidden_state
        elif isinstance(outputs, (tuple, list)) and len(outputs) > 0:
            last_hidden = outputs[0]
        else:
            raise RuntimeError("Cannot extract last_hidden_state from model output")
        
        # Mean poolingï¼ˆå¹³å‡æ± åŒ–ï¼Œå¿½ç•¥ paddingï¼‰
        mask = attention_mask.unsqueeze(-1).to(last_hidden.dtype)
        summed = (last_hidden * mask).sum(1)
        counts = mask.sum(1).clamp(min=1e-9)
        pooled = summed / counts
        
        # Lazy initialization of classifier
        if self.classifier is None:
            hidden_size = pooled.size(-1)
            self.classifier = nn.Linear(hidden_size, self.num_labels).to(pooled.device)
            print(f"âœ… Classifier initialized (hidden_size={hidden_size})")
        
        logits = self.classifier(pooled)
        
        # è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)
            return {"loss": loss, "logits": logits}
        
        return {"logits": logits}


# --------- Evaluation ---------
def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    if dataloader is None:
        return {}
    
    model.eval()
    all_labels = []
    all_preds = []
    all_probs = []
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs["logits"]
            
            probs = torch.softmax(logits, dim=-1)[:, 1]
            preds = logits.argmax(dim=-1)
            
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0
    cm = confusion_matrix(all_labels, all_preds).tolist()
    
    return {
        "acc": float(acc),
        "f1": float(f1),
        "auc": float(auc),
        "confusion_matrix": cm
    }


# --------- Main Training Loop ---------
def main():
    print("="*60)
    print("ğŸš€ Qwen2.5 Sentiment Classification Training")
    print("="*60)
    
    # 1. åŠ è½½æ•°æ®
    train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels = load_dataset()
    
    # 2. åŠ è½½ tokenizer
    print(f"ğŸ”„ Loading tokenizer from {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    
    # ç¡®ä¿æœ‰ pad_token
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("âš ï¸  Set pad_token = eos_token")
    
    # 3. åˆ›å»º DataLoader
    train_loader = make_dataloader(train_texts, train_labels, tokenizer, per_device_batch_size, shuffle=True)
    dev_loader = make_dataloader(dev_texts, dev_labels, tokenizer, per_device_batch_size, shuffle=False)
    test_loader = make_dataloader(test_texts, test_labels, tokenizer, per_device_batch_size, shuffle=False)
    
    # 4. åˆ›å»ºæ¨¡å‹
    model = QwenForSequenceClassification(MODEL_NAME, num_labels=2, cache_dir=CACHE_DIR)
    model.to(device)
    
    # è°ƒæ•´ token embeddingsï¼ˆå¦‚æœæ·»åŠ äº†æ–° tokenï¼‰
    if hasattr(model.base, "resize_token_embeddings"):
        model.base.resize_token_embeddings(len(tokenizer))
    
    # 5. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    no_decay = ["bias", "LayerNorm.weight", "LayerNorm.bias"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": weight_decay
        },
        {
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0
        },
    ]
    
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)
    
    total_steps = (len(train_loader) // gradient_accumulation_steps) * num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=max(1, total_steps // 10),
        num_training_steps=max(1, total_steps)
    )
    
    # 6. æ—¥å¿—æ–‡ä»¶
    log_file = os.path.join(LOG_DIR, f"qwen25_n{len(train_texts)}_seed{seed}.json")
    history = []
    if os.path.exists(log_file):
        with open(log_file, 'r') as f:
            history = json.load(f)
    
    # 7. è®­ç»ƒå¾ªç¯
    print("\n" + "="*60)
    print("ğŸ‹ï¸  Starting Training...")
    print("="*60)
    
    best_dev_f1 = 0.0
    global_step = 0
    
    for epoch in range(num_epochs):
        print(f"\nğŸ“… Epoch {epoch+1}/{num_epochs}")
        model.train()
        
        optimizer.zero_grad()
        running_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        
        for step, batch in enumerate(pbar):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs["loss"] / gradient_accumulation_steps
            loss.backward()
            running_loss += loss.item()
            
            if (step + 1) % gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                global_step += 1
                
                pbar.set_postfix({"loss": f"{running_loss:.4f}"})
                running_loss = 0.0
        
        # 8. éªŒè¯å’Œæµ‹è¯•
        print("\nğŸ“Š Evaluating...")
        dev_metrics = evaluate_model(model, dev_loader, device)
        test_metrics = evaluate_model(model, test_loader, device)
        
        if dev_metrics:
            print(f"  Dev  | Acc: {dev_metrics['acc']:.4f} | F1: {dev_metrics['f1']:.4f} | AUC: {dev_metrics['auc']:.4f}")
        if test_metrics:
            print(f"  Test | Acc: {test_metrics['acc']:.4f} | F1: {test_metrics['f1']:.4f} | AUC: {test_metrics['auc']:.4f}")
        
        # 9. ä¿å­˜æ—¥å¿—
        log_entry = {
            "epoch": epoch + 1,
            "sample_size": len(train_texts),
            "seed": seed,
            "dev_acc": dev_metrics.get("acc"),
            "dev_f1": dev_metrics.get("f1"),
            "dev_auc": dev_metrics.get("auc"),
            "test_acc": test_metrics.get("acc"),
            "test_f1": test_metrics.get("f1"),
            "test_auc": test_metrics.get("auc"),
            "confusion_matrix": dev_metrics.get("confusion_matrix")
        }
        
        history.append(log_entry)
        with open(log_file, 'w') as f:
            json.dump(history, f, indent=2)
        
        # 10. ä¿å­˜æ£€æŸ¥ç‚¹
        ckpt_dir = os.path.join(OUTPUT_DIR, f"checkpoint-epoch{epoch+1}")
        os.makedirs(ckpt_dir, exist_ok=True)
        
        try:
            model.base.save_pretrained(ckpt_dir)
        except Exception as e:
            print(f"âš ï¸  Cannot save base model with save_pretrained: {e}")
            torch.save(model.base.state_dict(), os.path.join(ckpt_dir, "base_state_dict.pt"))
        
        torch.save(model.classifier.state_dict(), os.path.join(ckpt_dir, "classifier.pt"))
        tokenizer.save_pretrained(ckpt_dir)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if dev_metrics and dev_metrics['f1'] > best_dev_f1:
            best_dev_f1 = dev_metrics['f1']
            best_ckpt_dir = os.path.join(OUTPUT_DIR, "best_model")
            os.makedirs(best_ckpt_dir, exist_ok=True)
            
            try:
                model.base.save_pretrained(best_ckpt_dir)
            except:
                torch.save(model.base.state_dict(), os.path.join(best_ckpt_dir, "base_state_dict.pt"))
            
            torch.save(model.classifier.state_dict(), os.path.join(best_ckpt_dir, "classifier.pt"))
            tokenizer.save_pretrained(best_ckpt_dir)
            print(f"ğŸ’¾ Best model saved (F1: {best_dev_f1:.4f})")
    
    print("\n" + "="*60)
    print("âœ… Training completed!")
    print(f"ğŸ“Š Best Dev F1: {best_dev_f1:.4f}")
    print(f"ğŸ’¾ Models saved to: {OUTPUT_DIR}")
    print(f"ğŸ“ Training log: {log_file}")
    print("="*60)


if __name__ == "__main__":
    main()

