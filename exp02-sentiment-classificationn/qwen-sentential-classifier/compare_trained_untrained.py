"""
è®­ç»ƒå‰åæ¨¡å‹å¯¹æ¯” - Qwenæƒ…æ„Ÿåˆ†ç±»å™¨ç‰ˆæœ¬
å¯¹æ¯”æœªè®­ç»ƒæ¨¡å‹å’Œå·²è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®ç‡å·®å¼‚
"""
import torch
import random
import argparse
import os
from transformers import AutoTokenizer
from config import Config
from model import SentimentClassifier
from load_data import DataLoader as DataLoaderClass
from tqdm import tqdm

def load_trained_model(device, config):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = SentimentClassifier(config.model_name, config.num_classes, freeze_base=True)
    model_path = config.model_save_path

    if os.path.exists(model_path):
        print(f"âœ“ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path}")
        model.load_state_dict(torch.load(model_path, map_location=device))
    else:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path}")

    model.to(device)
    model.eval()
    return model

def load_untrained_model(device, config):
    """åˆ›å»ºæœªè®­ç»ƒçš„æ¨¡å‹ï¼ˆéšæœºåˆå§‹åŒ–çš„åˆ†ç±»å±‚ï¼‰"""
    print(f"âœ“ åˆ›å»ºæœªè®­ç»ƒæ¨¡å‹ï¼ˆéšæœºåˆ†ç±»å™¨æƒé‡ï¼‰")
    model = SentimentClassifier(config.model_name, config.num_classes, freeze_base=True)
    model.to(device)
    model.eval()
    return model

def predict_batch(texts, model, tokenizer, device, config):
    """æ‰¹é‡é¢„æµ‹"""
    encoded = tokenizer.batch_encode_plus(
        texts,
        max_length=config.max_seq_length,
        add_special_tokens=True,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask)
        _, predictions = torch.max(outputs, dim=1)

    return predictions.cpu().numpy()

def evaluate_model(model, texts, labels, tokenizer, device, config, batch_size):
    """è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
    all_predictions = []
    sample_size = len(texts)

    for i in tqdm(range(0, sample_size, batch_size), desc="è¯„ä¼°ä¸­"):
        batch_texts = texts[i:i+batch_size]
        predictions = predict_batch(batch_texts, model, tokenizer, device, config)
        all_predictions.extend(predictions)

    correct = sum(1 for p, l in zip(all_predictions, labels) if p == l)
    accuracy = correct / sample_size

    return accuracy, correct, sample_size, all_predictions

def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯”è®­ç»ƒå‰åæ¨¡å‹å‡†ç¡®ç‡')
    parser.add_argument('--samples', type=int, default=1000,
                        help='æµ‹è¯•æ ·æœ¬æ•°é‡ (é»˜è®¤: 1000)')
    parser.add_argument('--full-test', action='store_true',
                        help='è¯„ä¼°å®Œæ•´æµ‹è¯•é›†ï¼ˆå¿½ç•¥--samplesï¼‰')
    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}\n")

    # åŠ è½½é…ç½®
    config = Config()

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_name,
        trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æµ‹è¯•æ•°æ®
    print("åŠ è½½æµ‹è¯•æ•°æ®...")
    data_loader = DataLoaderClass(config)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    test_file_path = os.path.join(current_dir, "test.csv")

    try:
        test_texts, test_labels = data_loader.load_csv(test_file_path)
    except FileNotFoundError:
        print(f"é”™è¯¯: æœªæ‰¾åˆ° {test_file_path}")
        return

    # é‡‡æ ·æµ‹è¯•æ•°æ®
    total_samples = len(test_texts)
    if args.full_test:
        sample_size = total_samples
        sampled_texts = test_texts
        sampled_labels = test_labels
        print(f"è¯„ä¼°å®Œæ•´æµ‹è¯•é›†: {sample_size} æ ·æœ¬\n")
    else:
        sample_size = min(args.samples, total_samples)
        print(f"ä» {total_samples} ä¸ªæ ·æœ¬ä¸­éšæœºæŠ½å– {sample_size} ä¸ª\n")
        indices = random.sample(range(total_samples), sample_size)
        sampled_texts = [test_texts[i] for i in indices]
        sampled_labels = [test_labels[i] for i in indices]

    # ==================== è¯„ä¼°æœªè®­ç»ƒæ¨¡å‹ ====================
    print("=" * 60)
    print("1ï¸âƒ£  è¯„ä¼°æœªè®­ç»ƒæ¨¡å‹ï¼ˆéšæœºæƒé‡ï¼‰")
    print("=" * 60)

    untrained_model = load_untrained_model(device, config)
    untrained_acc, untrained_correct, _, untrained_preds = evaluate_model(
        untrained_model, sampled_texts, sampled_labels, tokenizer, device, config, config.batch_size
    )

    print(f"\nğŸ“Š æœªè®­ç»ƒæ¨¡å‹ç»“æœ:")
    print(f"   å‡†ç¡®ç‡: {untrained_acc:.2%}")
    print(f"   æ­£ç¡®æ•°: {untrained_correct}/{sample_size}")

    # æ¸…ç†å†…å­˜
    del untrained_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # ==================== è¯„ä¼°è®­ç»ƒåæ¨¡å‹ ====================
    print("\n" + "=" * 60)
    print("2ï¸âƒ£  è¯„ä¼°è®­ç»ƒåæ¨¡å‹")
    print("=" * 60)

    trained_model = load_trained_model(device, config)
    trained_acc, trained_correct, _, trained_preds = evaluate_model(
        trained_model, sampled_texts, sampled_labels, tokenizer, device, config, config.batch_size
    )

    print(f"\nğŸ“Š è®­ç»ƒåæ¨¡å‹ç»“æœ:")
    print(f"   å‡†ç¡®ç‡: {trained_acc:.2%}")
    print(f"   æ­£ç¡®æ•°: {trained_correct}/{sample_size}")

    # ==================== å¯¹æ¯”ç»“æœ ====================
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å¯¹æ¯”ç»“æœ")
    print("=" * 60)
    print(f"æµ‹è¯•æ ·æœ¬æ•°:       {sample_size}")
    print(f"æœªè®­ç»ƒå‡†ç¡®ç‡:     {untrained_acc:.2%}  ({untrained_correct}/{sample_size})")
    print(f"è®­ç»ƒåå‡†ç¡®ç‡:     {trained_acc:.2%}  ({trained_correct}/{sample_size})")
    print(f"å‡†ç¡®ç‡æå‡:       {(trained_acc - untrained_acc):.2%}")
    if untrained_acc > 0:
        print(f"ç›¸å¯¹æå‡:         {((trained_acc - untrained_acc) / untrained_acc * 100):.1f}%")
    print("=" * 60)

    # ==================== é”™è¯¯æ¡ˆä¾‹åˆ†æ ====================
    print("\nğŸ“ é”™è¯¯æ¡ˆä¾‹åˆ†æ - è®­ç»ƒåæ¨¡å‹ï¼ˆå‰5ä¸ªé”™è¯¯ï¼‰:")
    error_count = 0
    for text, true_label, pred_label in zip(sampled_texts, sampled_labels, trained_preds):
        if true_label != pred_label:
            error_count += 1
            print(f"\n[é”™è¯¯ #{error_count}]")
            print(f"æ–‡æœ¬: {text[:150]}...")
            print(f"çœŸå®: {'æ­£é¢ (1)' if true_label==1 else 'è´Ÿé¢ (0)'} | "
                  f"é¢„æµ‹: {'æ­£é¢ (1)' if pred_label==1 else 'è´Ÿé¢ (0)'}")
            if error_count >= 5:
                break

    if error_count == 0:
        print("æœªå‘ç°é”™è¯¯ï¼å‡†ç¡®ç‡100%")

if __name__ == "__main__":
    main()
