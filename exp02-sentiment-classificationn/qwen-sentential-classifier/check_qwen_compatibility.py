"""
Qwenæ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥è„šæœ¬
åœ¨ä¿®æ”¹config.pyä¹‹å‰è¿è¡Œæ­¤è„šæœ¬ï¼Œç¡®ä¿Qwenæ¨¡å‹å¯ä»¥æ­£å¸¸åŠ è½½
"""
import torch
from transformers import AutoTokenizer, AutoModel
import warnings
warnings.filterwarnings('ignore')

def test_qwen_model(model_name):
    """æµ‹è¯•Qwenæ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸åŠ è½½å’Œä½¿ç”¨"""

    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"{'='*60}\n")

    try:
        # 1. æµ‹è¯•TokenizeråŠ è½½
        print("1ï¸âƒ£  æµ‹è¯•TokenizeråŠ è½½...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True  # Qwenéœ€è¦è¿™ä¸ªå‚æ•°
        )
        print(f"   âœ… TokenizeråŠ è½½æˆåŠŸ")
        print(f"   - Vocab size: {tokenizer.vocab_size}")
        print(f"   - Has pad_token: {tokenizer.pad_token is not None}")

        # å¦‚æœæ²¡æœ‰pad_tokenï¼Œè®¾ç½®ä¸ºeos_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f"   - è®¾ç½® pad_token = eos_token")

        # 2. æµ‹è¯•ModelåŠ è½½
        print("\n2ï¸âƒ£  æµ‹è¯•ModelåŠ è½½...")
        model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True  # Qwenéœ€è¦è¿™ä¸ªå‚æ•°
        )
        print(f"   âœ… ModelåŠ è½½æˆåŠŸ")
        print(f"   - Hidden size: {model.config.hidden_size}")
        print(f"   - Num layers: {model.config.num_hidden_layers}")

        # 3. æµ‹è¯•æ˜¯å¦æœ‰pooler_output
        print("\n3ï¸âƒ£  æµ‹è¯•æ¨¡å‹è¾“å‡ºæ ¼å¼...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        inputs = tokenizer(test_text, return_tensors="pt")

        with torch.no_grad():
            outputs = model(**inputs)

        has_pooler = hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None
        print(f"   - Has pooler_output: {has_pooler}")
        print(f"   - last_hidden_state shape: {outputs.last_hidden_state.shape}")

        if not has_pooler:
            print(f"   â„¹ï¸  Qwenæ²¡æœ‰pooler_outputï¼Œå°†ä½¿ç”¨ last_hidden_state[:, 0, :]")
            print(f"   âœ… ä»£ç å·²ç»å¤„ç†äº†è¿™ç§æƒ…å†µï¼ˆmodel.pyç¬¬52-53è¡Œï¼‰")

        # 4. æµ‹è¯•åˆ†ç±»å™¨å…¼å®¹æ€§
        print("\n4ï¸âƒ£  æµ‹è¯•åˆ†ç±»å™¨å…¼å®¹æ€§...")
        num_classes = 2
        classifier = torch.nn.Linear(model.config.hidden_size, num_classes)

        if has_pooler:
            pooled = outputs.pooler_output
        else:
            pooled = outputs.last_hidden_state[:, 0, :]

        logits = classifier(pooled)
        print(f"   âœ… åˆ†ç±»å™¨è¾“å‡ºshape: {logits.shape}")

        # 5. æµ‹è¯•å†…å­˜å ç”¨
        print("\n5ï¸âƒ£  æµ‹è¯•å†…å­˜å ç”¨...")
        model_size = sum(p.numel() for p in model.parameters()) / 1e6
        print(f"   - æ¨¡å‹å‚æ•°é‡: {model_size:.2f}M")

        if model_size > 1000:
            print(f"   âš ï¸  è­¦å‘Š: æ¨¡å‹è¾ƒå¤§ ({model_size:.2f}M)ï¼Œå»ºè®®ä½¿ç”¨GPUæˆ–å‡å°batch_size")

        print(f"\n{'='*60}")
        print("âœ… å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡ï¼å¯ä»¥å®‰å…¨ä½¿ç”¨æ­¤æ¨¡å‹")
        print(f"{'='*60}\n")

        return True, {
            'hidden_size': model.config.hidden_size,
            'vocab_size': tokenizer.vocab_size,
            'has_pooler': has_pooler,
            'model_size': model_size
        }

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        print(f"\nå¯èƒ½çš„åŸå› :")
        print(f"1. æ¨¡å‹åç§°é”™è¯¯")
        print(f"2. ç¼ºå°‘ä¾èµ–: pip install transformers_stream_generator")
        print(f"3. ç½‘ç»œè¿æ¥é—®é¢˜")
        print(f"4. éœ€è¦æ›´æ–°transformersç‰ˆæœ¬: pip install --upgrade transformers")
        return False, None

def recommend_config(model_name, model_info):
    """æ ¹æ®æ¨¡å‹ä¿¡æ¯æ¨èé…ç½®"""
    print("\nğŸ“‹ æ¨èçš„config.pyé…ç½®:")
    print("="*60)

    # æ ¹æ®æ¨¡å‹å¤§å°æ¨èbatch_size
    if model_info['model_size'] > 1000:
        batch_size = 4
    elif model_info['model_size'] > 500:
        batch_size = 8
    else:
        batch_size = 16

    # æ ¹æ®æ¨¡å‹æ¨èåºåˆ—é•¿åº¦
    if 'qwen' in model_name.lower():
        max_seq_length = 512  # Qwenæ”¯æŒæ›´é•¿åºåˆ—
    else:
        max_seq_length = 128

    config_template = f'''
class Config:
    """
    æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ‰€æœ‰å¯é…ç½®å‚æ•°
    """
    # æ¨¡å‹å‚æ•°
    model_name = "{model_name}"
    max_seq_length = {max_seq_length}  # Qwenæ”¯æŒæ›´é•¿åºåˆ—
    num_classes = 2

    # è®­ç»ƒå‚æ•°
    batch_size = {batch_size}  # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´
    learning_rate = 2e-5
    num_epochs = 5

    # è·¯å¾„é…ç½®
    train_path = "train.csv"
    dev_path = "dev.csv"
    test_path = "test.csv"
    model_save_path = "sentiment_model.pth"
'''

    print(config_template)
    print("="*60)

    print("\nâš ï¸  é‡è¦æç¤º:")
    print(f"1. éœ€è¦åœ¨ model.py å’Œ main.py ä¸­æ·»åŠ  trust_remote_code=True å‚æ•°")
    print(f"2. å»ºè®®batch_size={batch_size}ï¼ˆæ ¹æ®æ¨¡å‹å¤§å°{model_info['model_size']:.0f}Mè°ƒæ•´ï¼‰")
    print(f"3. åºåˆ—é•¿åº¦å¯ä»¥è®¾ä¸º{max_seq_length}ï¼ˆQwenæ”¯æŒæ›´é•¿åºåˆ—ï¼‰")
    if not model_info['has_pooler']:
        print(f"4. âœ… Qwenæ²¡æœ‰pooler_outputï¼Œä½†ä»£ç å·²è‡ªåŠ¨å¤„ç†")

if __name__ == "__main__":
    print("ğŸ” Qwenæ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥å·¥å…·\n")

    # å¸¸ç”¨çš„Qwenæ¨¡å‹åˆ—è¡¨
    qwen_models = [
        "Qwen/Qwen-1_8B",      # 1.8Bå‚æ•°ï¼Œè¾ƒå°
        "Qwen/Qwen-7B",        # 7Bå‚æ•°ï¼Œä¸­ç­‰
        "Qwen/Qwen-14B",       # 14Bå‚æ•°ï¼Œè¾ƒå¤§
        "Qwen/Qwen2-1.5B",     # Qwen2ç³»åˆ—
        "Qwen/Qwen2-7B",
    ]

    print("å¯ç”¨çš„Qwenæ¨¡å‹:")
    for i, model in enumerate(qwen_models, 1):
        print(f"  {i}. {model}")

    print("\nè¯·é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹ (è¾“å…¥æ•°å­—)ï¼Œæˆ–è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°:")
    user_input = input("> ").strip()

    # åˆ¤æ–­æ˜¯æ•°å­—è¿˜æ˜¯æ¨¡å‹åç§°
    if user_input.isdigit() and 1 <= int(user_input) <= len(qwen_models):
        model_name = qwen_models[int(user_input) - 1]
    else:
        model_name = user_input

    # è¿è¡Œæµ‹è¯•
    success, model_info = test_qwen_model(model_name)

    if success:
        recommend_config(model_name, model_info)

        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("1. ä¿®æ”¹ config.py ä¸­çš„ model_name")
        print("2. ä¿®æ”¹ model.py å’Œ main.py æ·»åŠ  trust_remote_code=True")
        print("3. è¿è¡Œè®­ç»ƒ: python main.py")
    else:
        print("\nâŒ å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥ï¼Œè¯·è§£å†³ä¸Šè¿°é—®é¢˜åé‡è¯•")
