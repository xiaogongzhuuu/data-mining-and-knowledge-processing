# BERT vs Qwen æ•°æ®è¾“å…¥å¯¹æ¯”åˆ†æ

## ğŸ“Š æ€»è§ˆå¯¹æ¯”è¡¨

| ç‰¹å¾ | BERT | Qwen | è¯´æ˜ |
|------|------|------|------|
| **Tokenizer** | `BertTokenizer` | `AutoTokenizer` | Qwen æ›´é€šç”¨ |
| **ç‰¹æ®Šæ ‡è®°** | `[CLS]`, `[SEP]`, `[PAD]` | åŠ¨æ€ï¼ˆæ¨¡å‹è‡ªå®šä¹‰ï¼‰ | BERT å›ºå®šï¼ŒQwen çµæ´» |
| **Padding Token** | è‡ªå¸¦ `[PAD]` | éœ€æ£€æŸ¥å¹¶è®¾ç½® | Qwen å¯èƒ½ç¼ºå¤± |
| **åºåˆ—ç»“æ„** | `[CLS] text [SEP]` | æ¨¡å‹ä¾èµ– | ç»“æ„ä¸åŒ |
| **è¾“å‡ºæ ¼å¼** | ç›¸åŒ | ç›¸åŒ | éƒ½è¿”å› input_ids, attention_mask, labels |

---

## ğŸ” è¯¦ç»†ä»£ç å¯¹æ¯”

### 1. Import å¯¼å…¥

#### BERT
```python
from transformers import BertTokenizer
```
- âœ… **ç‰¹å®š tokenizer**ï¼šä¸“é—¨ä¸º BERT è®¾è®¡
- âœ… **ç¨³å®šå¯é **ï¼šå›ºå®šçš„å®ç°
- âŒ **ä¸å¤Ÿçµæ´»**ï¼šåªèƒ½ç”¨äº BERT ç³»åˆ—

#### Qwen
```python
from transformers import AutoTokenizer
```
- âœ… **é€šç”¨ tokenizer**ï¼šå¯ä»¥è‡ªåŠ¨åŠ è½½ä»»ä½•æ¨¡å‹çš„ tokenizer
- âœ… **çµæ´»æ€§é«˜**ï¼šæ”¯æŒ BERTã€GPTã€Qwenã€LLaMA ç­‰
- âœ… **æœªæ¥å…¼å®¹**ï¼šæ›´æ¢æ¨¡å‹æ— éœ€ä¿®æ”¹ä»£ç 

**æ¨è**: Qwen çš„æ–¹å¼æ›´ç°ä»£ã€æ›´çµæ´»

---

### 2. åˆå§‹åŒ–ï¼ˆ__init__ï¼‰

#### BERT
```python
def __init__(self, texts, labels, tokenizer, max_len):
    self.texts = texts
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_len = max_len
```
- âœ… **ç®€æ´ç›´æ¥**ï¼šæ²¡æœ‰é¢å¤–å¤„ç†
- âš ï¸  **å‡è®¾å®Œæ•´**ï¼šå‡è®¾ tokenizer å·²æ­£ç¡®é…ç½®

#### Qwen
```python
def __init__(self, texts, labels, tokenizer, max_len):
    self.texts = texts
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_len = max_len
    
    # ç¡®ä¿tokenizeræœ‰padding token
    if self.tokenizer.pad_token is None:
        self.tokenizer.pad_token = self.tokenizer.eos_token
```
- âœ… **æ›´å¥å£®**ï¼šæ£€æŸ¥å¹¶ä¿®å¤ padding token
- âœ… **é˜²å¾¡æ€§ç¼–ç¨‹**ï¼šé¿å…è¿è¡Œæ—¶é”™è¯¯
- âœ… **å¤„ç†è¾¹ç•Œæƒ…å†µ**ï¼šQwen/GPT ç­‰è§£ç å™¨æ¨¡å‹å¯èƒ½æ²¡æœ‰ pad_token

**å…³é”®åŒºåˆ«**: Qwen æ·»åŠ äº† padding token æ£€æŸ¥

**ä¸ºä»€ä¹ˆéœ€è¦?**
```python
# BERT åŸç”Ÿå°±æœ‰ pad_token
BertTokenizer.from_pretrained('bert-base-uncased')
# pad_token = '[PAD]' âœ…

# Qwen åŸç”Ÿå¯èƒ½æ²¡æœ‰ pad_token
AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B')
# pad_token = None âŒ 
# éœ€è¦æ‰‹åŠ¨è®¾ç½®: tokenizer.pad_token = tokenizer.eos_token
```

---

### 3. æ•°æ®è·å–ï¼ˆ__getitem__ï¼‰

#### BERT
```python
# æ·»åŠ [CLS]å’Œ[SEP]æ ‡è®°
encoding = self.tokenizer.encode_plus(
    text,
    add_special_tokens=True,  # æ·»åŠ [CLS]å’Œ[SEP]æ ‡è®°
    max_length=self.max_len,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='pt'
)
```

#### Qwen
```python
# æ·»åŠ ç‰¹æ®Šæ ‡è®°
encoding = self.tokenizer.encode_plus(
    text,
    add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šæ ‡è®°
    max_length=self.max_len,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='pt'
)
```

**å¤–è§‚ç›¸åŒï¼Œä½†å†…éƒ¨ä¸åŒ**ï¼

---

## ğŸ”¬ æ·±å…¥åˆ†æï¼šç‰¹æ®Šæ ‡è®°çš„å·®å¼‚

### BERT çš„å¤„ç†æµç¨‹

```python
text = "I love this product"

# Tokenization
tokens = ['I', 'love', 'this', 'product']

# Add special tokens
tokens_with_special = ['[CLS]', 'I', 'love', 'this', 'product', '[SEP]']

# Convert to IDs
input_ids = [101, 1045, 2293, 2023, 3911, 102]
#           [CLS]  I   love  this product [SEP]

# Padding (å‡è®¾ max_length=10)
input_ids = [101, 1045, 2293, 2023, 3911, 102, 0, 0, 0, 0]
#           [CLS]  I   love  this product [SEP] [PAD][PAD][PAD][PAD]

# Attention mask
attention_mask = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
#                 æœ‰æ•ˆçš„å†…å®¹â†‘      å¡«å……çš„â†‘
```

**BERT åºåˆ—ç»“æ„**:
```
[CLS] + æ–‡æœ¬ tokens + [SEP] + [PAD]...
  â†‘                     â†‘        â†‘
åˆ†ç±»æ ‡è®°            å¥å­ç»“æŸ    å¡«å……
```

### Qwen çš„å¤„ç†æµç¨‹

```python
text = "I love this product"

# Tokenization (Qwen ä½¿ç”¨ BPE/Byte-level)
tokens = ['I', 'Ä love', 'Ä this', 'Ä product']  # Ä  è¡¨ç¤ºç©ºæ ¼

# Add special tokens (Qwen å¯èƒ½åªåœ¨å¼€å¤´æ·»åŠ  <|im_start|> ç­‰)
tokens_with_special = ['<|im_start|>', 'I', 'Ä love', 'Ä this', 'Ä product']

# Convert to IDs (IDå€¼å®Œå…¨ä¸åŒï¼)
input_ids = [151644, 40, 3986, 419, 2168]
#           <|start|> I  love this product

# Padding (ä½¿ç”¨ eos_token ä½œä¸º pad_token)
input_ids = [151644, 40, 3986, 419, 2168, 151643, 151643, 151643, 151643, 151643]
#           <|start|> I  love this product <|end|> <|end|> <|end|> ...

# Attention mask
attention_mask = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
```

**Qwen åºåˆ—ç»“æ„**:
```
<|im_start|> + æ–‡æœ¬ tokens + <|endoftext|>...
      â†‘                            â†‘
  å¼€å§‹æ ‡è®°                    ç»“æŸ/å¡«å……
```

---

## ğŸ’¡ å…³é”®å·®å¼‚æ€»ç»“

### 1. è¯è¡¨å¤§å°
```python
# BERT
vocab_size = 30,522  # ç›¸å¯¹è¾ƒå°ï¼Œä¸»è¦æ˜¯è‹±æ–‡ WordPiece

# Qwen
vocab_size = 151,643  # æ›´å¤§ï¼Œæ”¯æŒå¤šè¯­è¨€ Byte-level BPE
```

### 2. Token ID èŒƒå›´
```python
# BERT
[CLS] = 101
[SEP] = 102
[PAD] = 0

# Qwen
<|im_start|> = 151644
<|endoftext|> = 151643  # ä½œä¸º EOS å’Œ PAD
```

### 3. åˆ†è¯ç²’åº¦
```python
text = "unhappiness"

# BERT (WordPiece)
tokens = ['un', '##hap', '##pi', '##ness']
# åŸºäºå­è¯ï¼Œä½¿ç”¨ ## è¡¨ç¤ºéå¼€å¤´

# Qwen (BPE/Byte-level)
tokens = ['un', 'happiness']  # æˆ– ['unhap', 'piness']
# æ›´çµæ´»çš„å­—èŠ‚çº§ç¼–ç 
```

### 4. å¤šè¯­è¨€æ”¯æŒ
```python
text = "æˆ‘çˆ±è¿™ä¸ªäº§å“"

# BERT (éœ€è¦å¤šè¯­è¨€ç‰ˆæœ¬ bert-base-multilingual)
tokens = ['æˆ‘', 'çˆ±', 'è¿™', 'ä¸ª', 'äº§', 'å“']
# ä¸­æ–‡é€šå¸¸æŒ‰å­—åˆ†è¯

# Qwen (åŸç”Ÿæ”¯æŒä¸­æ–‡)
tokens = ['æˆ‘', 'çˆ±', 'è¿™ä¸ª', 'äº§å“']
# æ›´è‡ªç„¶çš„ä¸­æ–‡åˆ†è¯
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| ç‰¹å¾ | BERT | Qwen |
|------|------|------|
| **åˆ†è¯é€Ÿåº¦** | å¿« âš¡âš¡ | ä¸­ç­‰ âš¡ |
| **å†…å­˜å ç”¨** | å°è¯è¡¨ï¼Œä½å†…å­˜ ğŸ’¾ | å¤§è¯è¡¨ï¼Œé«˜å†…å­˜ ğŸ’¾ğŸ’¾ |
| **å¤šè¯­è¨€** | éœ€è¦ä¸“é—¨ç‰ˆæœ¬ ğŸŒ | åŸç”Ÿæ”¯æŒ ğŸŒğŸŒğŸŒ |
| **ç‰¹æ®Šæ ‡è®°** | å›ºå®šï¼Œç®€å• âœ… | çµæ´»ï¼Œéœ€é…ç½® âš™ï¸ |

---

## ğŸ¯ å®é™…è¾“å…¥è¾“å‡ºç¤ºä¾‹

### è¾“å…¥æ–‡æœ¬
```python
text = "This product is amazing! I love it."
```

### BERT å¤„ç†ç»“æœ
```python
{
    'input_ids': tensor([
        101,    # [CLS]
        2023,   # This
        3234,   # product
        2003,   # is
        6429,   # amazing
        999,    # !
        1045,   # I
        2293,   # love
        2009,   # it
        1012,   # .
        102,    # [SEP]
        0, 0, 0, 0, ...  # [PAD]
    ]),
    'attention_mask': tensor([1,1,1,1,1,1,1,1,1,1,1, 0,0,0,0,...]),
    'labels': tensor(1)  # æ­£é¢
}
```

### Qwen å¤„ç†ç»“æœ
```python
{
    'input_ids': tensor([
        151644, # <|im_start|>
        2028,   # This
        2652,   # Ä product
        374,    # Ä is
        8056,   # Ä amazing
        0,      # !
        358,    # Ä I
        3021,   # Ä love
        433,    # Ä it
        13,     # .
        151643, # <|endoftext|> (padding)
        151643, 151643, ...
    ]),
    'attention_mask': tensor([1,1,1,1,1,1,1,1,1,1, 0,0,0,...]),
    'labels': tensor(1)  # æ­£é¢
}
```

---

## ğŸ”§ ä»£ç å…¼å®¹æ€§

### å¥½æ¶ˆæ¯ âœ…
ä¸¤ç§æ•°æ®é›†ç±»çš„**æ¥å£å®Œå…¨å…¼å®¹**ï¼

```python
# åˆ›å»º BERT æ•°æ®é›†
bert_dataset = SentimentDataset(texts, labels, bert_tokenizer, max_len=128)

# åˆ›å»º Qwen æ•°æ®é›†
qwen_dataset = SentimentDataset(texts, labels, qwen_tokenizer, max_len=128)

# ä½¿ç”¨æ–¹å¼å®Œå…¨ç›¸åŒ
dataloader = DataLoader(bert_dataset, batch_size=32)  # æˆ– qwen_dataset
```

### è¿”å›æ ¼å¼ç›¸åŒ
```python
batch = next(iter(dataloader))
# æ— è®ºæ˜¯ BERT è¿˜æ˜¯ Qwenï¼Œéƒ½è¿”å›ï¼š
{
    'input_ids': Tensor[batch_size, seq_len],
    'attention_mask': Tensor[batch_size, seq_len],
    'labels': Tensor[batch_size]
}
```

---

## ğŸ“ æœ€ä½³å®è·µå»ºè®®

### 1. Tokenizer é€‰æ‹©
```python
# âœ… æ¨èï¼šä½¿ç”¨ AutoTokenizerï¼ˆæ›´çµæ´»ï¼‰
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
# æˆ–
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B')
```

### 2. Padding Token æ£€æŸ¥
```python
# âœ… æ¨èï¼šæ€»æ˜¯æ£€æŸ¥ padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    # æˆ–è®¾ç½®ä¸ºç‰¹å®šå€¼
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})
```

### 3. åºåˆ—é•¿åº¦è®¾ç½®
```python
# BERT: é€šå¸¸ 512 æ˜¯æœ€å¤§å€¼
max_len_bert = 512

# Qwen: æ ¹æ®æ¨¡å‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ›´é•¿
max_len_qwen = 2048  # æˆ– 4096, 8192ï¼ˆå–å†³äºæ¨¡å‹ï¼‰
```

### 4. æ‰¹å¤„ç†ç­–ç•¥
```python
# âœ… æ¨èï¼šä½¿ç”¨ DataLoader çš„ collate_fn
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
dataloader = DataLoader(dataset, collate_fn=data_collator, batch_size=32)
# å¯ä»¥åŠ¨æ€å¡«å……ï¼Œä¸æµªè´¹å†…å­˜
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹ Token åŒ–ç»“æœ
```python
text = "I love this product"

# BERT
tokens = bert_tokenizer.tokenize(text)
print(tokens)  # ['i', 'love', 'this', 'product']

ids = bert_tokenizer.encode(text)
print(ids)  # [101, 1045, 2293, 2023, 3911, 102]

decoded = bert_tokenizer.decode(ids)
print(decoded)  # '[CLS] i love this product [SEP]'

# Qwen
tokens = qwen_tokenizer.tokenize(text)
print(tokens)  # ['I', 'Ä love', 'Ä this', 'Ä product']

ids = qwen_tokenizer.encode(text)
print(ids)  # [151644, 40, 3986, 419, 2168, 151643]

decoded = qwen_tokenizer.decode(ids)
print(decoded)  # '<|im_start|>I love this product<|endoftext|>'
```

### æ£€æŸ¥ç‰¹æ®Š Token
```python
print("BERT:")
print(f"  CLS token: {bert_tokenizer.cls_token} (ID: {bert_tokenizer.cls_token_id})")
print(f"  SEP token: {bert_tokenizer.sep_token} (ID: {bert_tokenizer.sep_token_id})")
print(f"  PAD token: {bert_tokenizer.pad_token} (ID: {bert_tokenizer.pad_token_id})")

print("\nQwen:")
print(f"  BOS token: {qwen_tokenizer.bos_token} (ID: {qwen_tokenizer.bos_token_id})")
print(f"  EOS token: {qwen_tokenizer.eos_token} (ID: {qwen_tokenizer.eos_token_id})")
print(f"  PAD token: {qwen_tokenizer.pad_token} (ID: {qwen_tokenizer.pad_token_id})")
```

---

## ğŸ“ æ€»ç»“

### ç›¸åŒç‚¹ âœ…
1. **æ¥å£ä¸€è‡´**ï¼šéƒ½ç»§æ‰¿ `Dataset`ï¼Œå®ç°ç›¸åŒçš„æ–¹æ³•
2. **è¾“å‡ºæ ¼å¼**ï¼šéƒ½è¿”å› `input_ids`, `attention_mask`, `labels`
3. **å¤„ç†æµç¨‹**ï¼štokenize â†’ add_special_tokens â†’ padding â†’ truncation
4. **ä½¿ç”¨æ–¹å¼**ï¼šä¸ DataLoader é…åˆä½¿ç”¨æ–¹å¼ç›¸åŒ

### ä¸åŒç‚¹ âš ï¸
1. **Tokenizer ç±»å‹**ï¼š`BertTokenizer` vs `AutoTokenizer`
2. **ç‰¹æ®Šæ ‡è®°**ï¼š`[CLS][SEP][PAD]` vs `<|im_start|><|endoftext|>`
3. **è¯è¡¨å¤§å°**ï¼š30K vs 150K
4. **Padding å¤„ç†**ï¼šBERT è‡ªå¸¦ï¼ŒQwen éœ€è¦é…ç½®
5. **åˆ†è¯ç®—æ³•**ï¼šWordPiece vs BPE/Byte-level

### æ¨èå®è·µ ğŸŒŸ
1. âœ… ä½¿ç”¨ `AutoTokenizer`ï¼ˆæ›´çµæ´»ï¼‰
2. âœ… å§‹ç»ˆæ£€æŸ¥ `pad_token`ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
3. âœ… ç»Ÿä¸€æ•°æ®é›†æ¥å£ï¼ˆä¾¿äºåˆ‡æ¢æ¨¡å‹ï¼‰
4. âœ… ä½¿ç”¨åŠ¨æ€å¡«å……ï¼ˆèŠ‚çœå†…å­˜ï¼‰

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- BERT Dataset: `bert-sentential-classifer/dataset.py`
- Qwen Dataset: `qwen-sentential-classifier/dataset.py`
- TextCNN Dataset: `textcnn-sentiment-classifier/data_loader.py` (å®Œå…¨ä¸åŒçš„å®ç°)

---

**ç»“è®º**: è™½ç„¶è¡¨é¢çœ‹èµ·æ¥å‡ ä¹ç›¸åŒï¼Œä½† BERT å’Œ Qwen åœ¨åº•å±‚çš„ tokenization å’Œç‰¹æ®Šæ ‡è®°å¤„ç†ä¸Šæœ‰æ˜¾è‘—å·®å¼‚ã€‚Qwen çš„å®ç°æ›´å¥å£®ï¼ˆæœ‰ padding token æ£€æŸ¥ï¼‰ï¼Œè€Œ BERT çš„å®ç°æ›´ç®€æ´ï¼ˆå› ä¸º tokenizer è‡ªå¸¦æ‰€æœ‰å¿…è¦é…ç½®ï¼‰ã€‚

