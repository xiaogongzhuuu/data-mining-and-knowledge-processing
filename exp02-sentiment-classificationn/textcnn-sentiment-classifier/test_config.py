"""
æµ‹è¯•é…ç½®è„šæœ¬ï¼šéªŒè¯æ•°æ®åŠ è½½å’Œå‡è¡¡é‡‡æ ·
"""

import config
from data_loader import create_data_loaders

print("="*80)
print("TextCNN é…ç½®æµ‹è¯•")
print("="*80)

print("\nğŸ“‹ å½“å‰é…ç½®:")
print(f"  MAX_TRAIN_SAMPLES: {config.MAX_TRAIN_SAMPLES}")
print(f"  BALANCE_TRAIN_DATA: {config.BALANCE_TRAIN_DATA}")
print(f"  BATCH_SIZE: {config.BATCH_SIZE}")
print(f"  NUM_EPOCHS: {config.NUM_EPOCHS}")
print(f"  EMBEDDING_DIM: {config.EMBEDDING_DIM}")
print(f"  NUM_FILTERS: {config.NUM_FILTERS}")
print(f"  FILTER_SIZES: {config.FILTER_SIZES}")
print(f"  MAX_SEQ_LENGTH: {config.MAX_SEQ_LENGTH}")

print("\n" + "="*80)
print("æµ‹è¯•æ•°æ®åŠ è½½...")
print("="*80)

try:
    train_loader, dev_loader, test_loader, vocab = create_data_loaders()
    
    print("\nâœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
    print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(dev_loader.dataset)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬")
    print(f"  è¯è¡¨å¤§å°: {len(vocab)}")
    
    # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒ
    from collections import Counter
    train_labels = [train_loader.dataset[i]['labels'].item() for i in range(len(train_loader.dataset))]
    train_dist = Counter(train_labels)
    
    print(f"\nâš–ï¸  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"  è´Ÿé¢ (0): {train_dist[0]} ({train_dist[0]/len(train_labels)*100:.1f}%)")
    print(f"  æ­£é¢ (1): {train_dist[1]} ({train_dist[1]/len(train_labels)*100:.1f}%)")
    
    if abs(train_dist[0] - train_dist[1]) < 100:
        print(f"  âœ… æ ·æœ¬åˆ†å¸ƒå‡è¡¡ï¼")
    else:
        print(f"  âš ï¸  æ ·æœ¬åˆ†å¸ƒä¸å‡è¡¡")
    
    print("\n" + "="*80)
    print("æµ‹è¯•å®Œæˆï¼å‡†å¤‡å¼€å§‹è®­ç»ƒã€‚")
    print("="*80)
    print("\nè¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
    print("  python train.py")
    
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()


