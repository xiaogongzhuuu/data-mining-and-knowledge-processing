"""
æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ¨¡å—
"""

import re
import csv
import pickle
from collections import Counter
from typing import List, Tuple, Dict
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

import config


def clean_text(text: str) -> str:
    """
    æ–‡æœ¬æ¸…æ´—ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ã€å¤šä½™ç©ºæ ¼ç­‰
    """
    # è½¬å°å†™
    text = text.lower()
    
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', ' ', text)
    
    # å»é™¤URL
    text = re.sub(r'http\S+|www\S+', ' ', text)
    
    # ä¿ç•™å­—æ¯ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^a-z0-9\s.!?,\'-]', ' ', text)
    
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()


def simple_tokenize(text: str) -> List[str]:
    """
    ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²
    """
    text = clean_text(text)
    tokens = text.split()
    return tokens


def load_data_from_csv(filepath: str, max_samples: int = None) -> Tuple[List[str], List[int]]:
    """
    ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®
    æ ¼å¼: label, title, text
    """
    texts = []
    labels = []
    
    print(f"Loading data from {filepath}...")
    
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        reader = csv.reader(f)
        for i, row in enumerate(reader):
            if max_samples and i >= max_samples:
                break
            
            if len(row) < 2:
                continue
            
            try:
                # è¯»å–æ ‡ç­¾
                label = int(row[0].strip())
                
                # åˆå¹¶ title å’Œ text
                text = ' '.join(r.strip() for r in row[1:] if r.strip())
                
                if not text:
                    continue
                
                # æ ‡ç­¾æ˜ å°„: 1(è´Ÿé¢)->0, 2(æ­£é¢)->1
                mapped_label = 0 if label == 1 else 1
                
                texts.append(text)
                labels.append(mapped_label)
                
            except (ValueError, IndexError):
                continue
    
    print(f"  Loaded {len(texts)} samples")
    return texts, labels


class Vocabulary:
    """
    è¯è¡¨ç±»
    """
    def __init__(self):
        self.word2idx = {}
        self.idx2word = {}
        self.word_freq = Counter()
        
        # ç‰¹æ®Šæ ‡è®°
        self.pad_token = config.PADDING_TOKEN
        self.unk_token = config.UNKNOWN_TOKEN
        
        # åˆå§‹åŒ–ç‰¹æ®Šæ ‡è®°
        self.word2idx[self.pad_token] = 0
        self.word2idx[self.unk_token] = 1
        self.idx2word[0] = self.pad_token
        self.idx2word[1] = self.unk_token
    
    def build_from_texts(self, texts: List[str], min_freq: int = 2, max_size: int = 50000):
        """
        ä»æ–‡æœ¬æ„å»ºè¯è¡¨
        """
        print("\nBuilding vocabulary...")
        
        # ç»Ÿè®¡è¯é¢‘
        for text in texts:
            tokens = simple_tokenize(text)
            self.word_freq.update(tokens)
        
        print(f"  Total unique words: {len(self.word_freq)}")
        
        # æŒ‰è¯é¢‘æ’åºï¼Œä¿ç•™é«˜é¢‘è¯
        most_common = self.word_freq.most_common(max_size)
        
        # è¿‡æ»¤ä½é¢‘è¯
        idx = len(self.word2idx)
        for word, freq in most_common:
            if freq >= min_freq:
                if word not in self.word2idx:
                    self.word2idx[word] = idx
                    self.idx2word[idx] = word
                    idx += 1
        
        print(f"  Vocabulary size: {len(self.word2idx)} (min_freq={min_freq})")
        return self
    
    def encode(self, text: str) -> List[int]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•åºåˆ—
        """
        tokens = simple_tokenize(text)
        indices = [self.word2idx.get(token, self.word2idx[self.unk_token]) 
                   for token in tokens]
        return indices
    
    def decode(self, indices: List[int]) -> str:
        """
        å°†ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬
        """
        words = [self.idx2word.get(idx, self.unk_token) for idx in indices]
        return ' '.join(words)
    
    def __len__(self):
        return len(self.word2idx)
    
    def save(self, filepath: str):
        """ä¿å­˜è¯è¡¨"""
        with open(filepath, 'wb') as f:
            pickle.dump(self, f)
        print(f"Vocabulary saved to {filepath}")
    
    @staticmethod
    def load(filepath: str):
        """åŠ è½½è¯è¡¨"""
        with open(filepath, 'rb') as f:
            vocab = pickle.load(f)
        print(f"Vocabulary loaded from {filepath}")
        return vocab


class SentimentDataset(Dataset):
    """
    æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†
    """
    def __init__(self, texts: List[str], labels: List[int], vocab: Vocabulary, max_length: int):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # ç¼–ç æ–‡æœ¬
        indices = self.vocab.encode(text)
        
        # æˆªæ–­æˆ–å¡«å……
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices = indices + [self.vocab.word2idx[self.vocab.pad_token]] * (self.max_length - len(indices))
        
        return {
            'input_ids': torch.tensor(indices, dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long),
            'length': torch.tensor(min(len(self.vocab.encode(text)), self.max_length), dtype=torch.long)
        }


def create_data_loaders(vocab: Vocabulary = None):
    """
    åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    # åŠ è½½æ•°æ®ï¼ˆå…ˆä¸é™åˆ¶æ•°é‡ï¼‰
    train_texts, train_labels = load_data_from_csv(config.TRAIN_FILE, max_samples=None)
    dev_texts, dev_labels = load_data_from_csv(config.DEV_FILE)
    test_texts, test_labels = load_data_from_csv(config.TEST_FILE)
    
    # å‡è¡¡é‡‡æ ·è®­ç»ƒé›†
    if config.MAX_TRAIN_SAMPLES and len(train_texts) > config.MAX_TRAIN_SAMPLES:
        if hasattr(config, 'BALANCE_TRAIN_DATA') and config.BALANCE_TRAIN_DATA:
            print(f"\nâš–ï¸  Balanced sampling {config.MAX_TRAIN_SAMPLES} training samples...")
            
            import random
            # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
            neg_indices = [i for i, label in enumerate(train_labels) if label == 0]
            pos_indices = [i for i, label in enumerate(train_labels) if label == 1]
            
            # æ¯ç±»é‡‡æ ·ä¸€åŠ
            samples_per_class = config.MAX_TRAIN_SAMPLES // 2
            
            rng = random.Random(42)
            rng.shuffle(neg_indices)
            rng.shuffle(pos_indices)
            
            selected_neg = neg_indices[:samples_per_class]
            selected_pos = pos_indices[:samples_per_class]
            
            # åˆå¹¶å¹¶æ‰“ä¹±
            selected_indices = selected_neg + selected_pos
            rng.shuffle(selected_indices)
            
            train_texts = [train_texts[i] for i in selected_indices]
            train_labels = [train_labels[i] for i in selected_indices]
            
            print(f"   âœ… Sampled {len(selected_neg)} negative + {len(selected_pos)} positive = {len(train_texts)} total")
        else:
            # éšæœºé‡‡æ ·ï¼ˆä¸å‡è¡¡ï¼‰
            train_texts = train_texts[:config.MAX_TRAIN_SAMPLES]
            train_labels = train_labels[:config.MAX_TRAIN_SAMPLES]
            print(f"\nğŸš€ Sampled {config.MAX_TRAIN_SAMPLES} training samples")
    
    # æ„å»ºæˆ–åŠ è½½è¯è¡¨
    if vocab is None:
        vocab = Vocabulary()
        vocab.build_from_texts(train_texts, config.MIN_WORD_FREQ, config.MAX_VOCAB_SIZE)
        vocab.save(config.VOCAB_SAVE_PATH)
    
    # æ‰“å°æ•°æ®ç»Ÿè®¡
    print("\n" + "="*60)
    print("Dataset Statistics:")
    print(f"  Train: {len(train_texts)} samples (neg: {train_labels.count(0)}, pos: {train_labels.count(1)})")
    print(f"  Dev:   {len(dev_texts)} samples (neg: {dev_labels.count(0)}, pos: {dev_labels.count(1)})")
    print(f"  Test:  {len(test_texts)} samples (neg: {test_labels.count(0)}, pos: {test_labels.count(1)})")
    print(f"  Vocabulary size: {len(vocab)}")
    print(f"  Max sequence length: {config.MAX_SEQ_LENGTH}")
    print("="*60)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SentimentDataset(train_texts, train_labels, vocab, config.MAX_SEQ_LENGTH)
    dev_dataset = SentimentDataset(dev_texts, dev_labels, vocab, config.MAX_SEQ_LENGTH)
    test_dataset = SentimentDataset(test_texts, test_labels, vocab, config.MAX_SEQ_LENGTH)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
    dev_loader = DataLoader(dev_dataset, batch_size=config.BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)
    
    return train_loader, dev_loader, test_loader, vocab


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    train_loader, dev_loader, test_loader, vocab = create_data_loaders()
    
    # æ‰“å°ä¸€ä¸ªbatchçš„ç¤ºä¾‹
    for batch in train_loader:
        print("\nSample batch:")
        print(f"  input_ids shape: {batch['input_ids'].shape}")
        print(f"  labels shape: {batch['labels'].shape}")
        print(f"  First sample:")
        print(f"    indices: {batch['input_ids'][0][:20].tolist()}...")
        print(f"    label: {batch['labels'][0].item()}")
        print(f"    decoded: {vocab.decode(batch['input_ids'][0][:20].tolist())}")
        break

